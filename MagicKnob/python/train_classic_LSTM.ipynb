{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1626,"status":"ok","timestamp":1688826587675,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"AUk8TQCYzAAe","outputId":"f75d105e-2f79-45ec-8eb6-104f65d1441c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/Othercomputers/Il mio MacBook Pro/MagicKnob\n","/content/drive/Othercomputers/Il mio MacBook Pro/MagicKnob/python\n"]}],"source":["import os\n","from datetime import datetime\n","\n","drive = True\n","\n","if drive:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    %cd /content/drive/Othercomputers/Il mio MacBook Pro/MagicKnob\n","\n","    path = \"/content/drive/Othercomputers/Il mio MacBook Pro/MagicKnob/\"\n","\n","    # check python file folder\n","    assert os.path.exists(path + \"python\"), f\"Upload python files in {path}python\"\n","    %cd ./python\n","\n","    # check data folder\n","    assert os.path.exists(path + \"data\"), f\"Upload data files in {path}data\"\n","else:\n","    path = \"../\"\n","\n","    # check python file folder\n","    assert os.path.exists(path + \"python\"), f\"Upload python files in {path}python\"\n","\n","    # check data folder\n","    assert os.path.exists(path + \"data\"), f\"Upload data files in {path}data\""]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1860,"status":"ok","timestamp":1688826589533,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"Wgz6WkLZzHDM"},"outputs":[],"source":["import myk_data\n","import myk_models\n","import myk_loss\n","import myk_train\n","import myk_evaluate\n","\n","import torch\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5604,"status":"ok","timestamp":1688826595134,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"mX_cQAvgzMLR"},"outputs":[],"source":["from torch.utils.tensorboard import SummaryWriter"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1688826595134,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"FXrHBuv6zQPg"},"outputs":[],"source":["# used for the writing of example outputs\n","run_name=\"audio_lstm_lpf\"\n","# dataset : need an input and output folder in this folder\n","audio_folder = f\"../data/{run_name}\"\n","#audio_folder = \"../../data/audio_ht1\"\n","assert os.path.exists(audio_folder), f\"Audio folder  not found. Looked for {audio_folder}\"\n","# used to render example output during training\n","test_file = \"../data/guitar.wav\"\n","assert os.path.exists(test_file), \"Test file not found. Looked for \" + test_file"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1688826595135,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"wJPGiuLV0ylP"},"outputs":[],"source":["# initialize net specs\n","lstm_hidden_size = 64\n","learning_rate = 5e-3\n","batch_size = 50\n","max_epochs = 10000\n","\n","# create the logger for tensorboard\n","writer = SummaryWriter()"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1688826595135,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"izzFWaHj0zej","outputId":"31c9ca80-4694-4944-df4c-f416d04edcf2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading dataset from folder  ../data/audio_lstm_lpf\n","loading input and output of ht1\n","    loading output of ht1\n","    generate_dataset:: Loaded frames from audio file 22050\n","    found input fragments of shape (120, 22050, 1)\n","    found output fragments of shape (120, 22050, 1)\n","    total input shape: (120, 22050, 1)\n","    total output shape: (120, 22050, 1)\n","\n","total input tensor shape torch.Size([120, 22050, 1])\n","total output tensor shape torch.Size([120, 22050, 1])\n","Splitting dataset\n"]}],"source":["print(\"Loading dataset from folder \", audio_folder)\n","dataset = myk_data.generate_dataset(audio_folder + \"/input/\", audio_folder + \"/output/\", frag_len_seconds=0.5)\n","\n","print(\"Splitting dataset\")\n","train_ds, val_ds, test_ds = myk_data.get_train_valid_test_datasets(dataset)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1688826595135,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"p5wnmGGzU1KX","outputId":"bfcbf589-6b67-4387-f0ba-e16ff29432a3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[-3.0518e-05],\n","         [-3.0518e-05],\n","         [ 0.0000e+00],\n","         ...,\n","         [ 0.0000e+00],\n","         [ 0.0000e+00],\n","         [ 0.0000e+00]]),\n"," tensor([[ 0.0000e+00],\n","         [ 0.0000e+00],\n","         [ 0.0000e+00],\n","         ...,\n","         [-1.4067e-05],\n","         [-1.3947e-05],\n","         [-1.3947e-05]]))"]},"metadata":{},"execution_count":7}],"source":["dataset[0]"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1688826595136,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"KpY8DxYl1Cb0","outputId":"59899f3c-677c-4b41-87fb-52d607544eb7"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda device available\n"]}],"source":["# test GPU, must be done after splitting\n","device = myk_train.get_device()"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":385,"status":"ok","timestamp":1688826595503,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"D3WztACX2UOd"},"outputs":[],"source":["# create data loaders\n","train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=device))\n","val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=device))\n","test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=device))"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":4468,"status":"ok","timestamp":1688826699931,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"8JZL-Qqa2ORT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"69903697-44be-4267-9fae-b9423bac338e"},"outputs":[{"output_type":"stream","name":"stdout","text":["SimpleLSTM(\n","  (lstm): LSTM(1, 64, batch_first=True)\n","  (dense): Linear(in_features=64, out_features=1, bias=True)\n",")\n"]}],"source":["model = myk_models.SimpleLSTM(hidden_size=lstm_hidden_size).to(device)\n","print(model)"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":249,"status":"ok","timestamp":1688826721279,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"4CuDKuR92cxv"},"outputs":[],"source":["# crate optimizer and loss function\n","optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n","#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', factor=0.5, patience=5, verbose=True) non lo usava\n","\n","loss_functions = myk_loss.LossWrapper()\n","\n","# https://github.com/Alec-Wright/Automated-GuitarAmpModelling/blob/main/dist_model_recnet.py\n","# https://github.com/Alec-Wright/CoreAudioML/blob/bad9469f94a2fa63a50d70ff75f5eff2208ba03f/training.py"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1688826721637,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"-6cmEvsqHwL7"},"outputs":[],"source":["#%load_ext tensorboard\n","#%tensorboard --logdir logs"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":765800,"status":"ok","timestamp":1688827487742,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"L2_vgpXU2tvW","outputId":"9526f35d-4ac4-4c42-8b0a-35208c74843c"},"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 0, train_loss 0.7276068925857544, val_loss 0.6470811367034912 \n","    Record loss - saving at epoch 1\n","    epoch 1, train_loss 0.6044882535934448, val_loss 0.5986725687980652 \n","    Record loss - saving at epoch 2\n","    epoch 2, train_loss 0.596645176410675, val_loss 0.5832301378250122 \n","    Record loss - saving at epoch 3\n","    epoch 3, train_loss 0.5917901396751404, val_loss 0.5825192332267761 \n","    Record loss - saving at epoch 4\n","    epoch 4, train_loss 0.5860413312911987, val_loss 0.581730306148529 \n","    Record loss - saving at epoch 6\n","    epoch 6, train_loss 0.5816167593002319, val_loss 0.5811533331871033 \n","    Record loss - saving at epoch 9\n","    epoch 9, train_loss 0.5756134390830994, val_loss 0.5789920091629028 \n","    Record loss - saving at epoch 10\n","    epoch 10, train_loss 0.5901691317558289, val_loss 0.5756321549415588 \n","    Record loss - saving at epoch 17\n","    epoch 17, train_loss 0.5668160915374756, val_loss 0.5739730596542358 \n","    Record loss - saving at epoch 20\n","    epoch 20, train_loss 0.5780108571052551, val_loss 0.5729064345359802 \n","    Record loss - saving at epoch 48\n","    epoch 48, train_loss 0.5690361261367798, val_loss 0.5707827210426331 \n","epoch 50, train_loss 0.5766629576683044, val_loss 0.587588906288147 \n","    Record loss - saving at epoch 66\n","    epoch 66, train_loss 0.5736210346221924, val_loss 0.5707315802574158 \n","    Record loss - saving at epoch 79\n","    epoch 79, train_loss 0.5690560936927795, val_loss 0.5702235102653503 \n","    Record loss - saving at epoch 91\n","    epoch 91, train_loss 0.562402606010437, val_loss 0.5696519613265991 \n","    Record loss - saving at epoch 94\n","    epoch 94, train_loss 0.5676854252815247, val_loss 0.5693894028663635 \n","epoch 100, train_loss 0.5668443441390991, val_loss 0.572372317314148 \n","    Record loss - saving at epoch 103\n","    epoch 103, train_loss 0.5674534440040588, val_loss 0.568747341632843 \n","    Record loss - saving at epoch 113\n","    epoch 113, train_loss 0.5666744709014893, val_loss 0.5683050155639648 \n","    Record loss - saving at epoch 121\n","    epoch 121, train_loss 0.5593292117118835, val_loss 0.5668821930885315 \n","    Record loss - saving at epoch 122\n","    epoch 122, train_loss 0.5606116056442261, val_loss 0.5638691782951355 \n","    Record loss - saving at epoch 129\n","    epoch 129, train_loss 0.5643988251686096, val_loss 0.5587238073348999 \n","    Record loss - saving at epoch 131\n","    epoch 131, train_loss 0.5568772554397583, val_loss 0.5564896464347839 \n","    Record loss - saving at epoch 134\n","    epoch 134, train_loss 0.5595429539680481, val_loss 0.5561695694923401 \n","    Record loss - saving at epoch 145\n","    epoch 145, train_loss 0.555385947227478, val_loss 0.5559287071228027 \n","    Record loss - saving at epoch 148\n","    epoch 148, train_loss 0.5545262098312378, val_loss 0.5492001175880432 \n","    Record loss - saving at epoch 149\n","    epoch 149, train_loss 0.554427981376648, val_loss 0.546602725982666 \n","epoch 150, train_loss 0.5506635904312134, val_loss 0.5653083324432373 \n","    Record loss - saving at epoch 172\n","    epoch 172, train_loss 0.5497146844863892, val_loss 0.5461808443069458 \n","    Record loss - saving at epoch 178\n","    epoch 178, train_loss 0.5549148321151733, val_loss 0.5448506474494934 \n","    Record loss - saving at epoch 190\n","    epoch 190, train_loss 0.5496045351028442, val_loss 0.541278600692749 \n","epoch 200, train_loss 0.559352457523346, val_loss 0.5490859746932983 \n","    Record loss - saving at epoch 235\n","    epoch 235, train_loss 0.5497869849205017, val_loss 0.5411191582679749 \n","    Record loss - saving at epoch 237\n","    epoch 237, train_loss 0.5426764488220215, val_loss 0.5401583313941956 \n","epoch 250, train_loss 0.55128014087677, val_loss 0.5439310073852539 \n","    Record loss - saving at epoch 260\n","    epoch 260, train_loss 0.5469275712966919, val_loss 0.5392580628395081 \n","    Record loss - saving at epoch 266\n","    epoch 266, train_loss 0.5423115491867065, val_loss 0.5377590656280518 \n","    Record loss - saving at epoch 291\n","    epoch 291, train_loss 0.5459117889404297, val_loss 0.5371631383895874 \n","    Record loss - saving at epoch 300\n","    epoch 300, train_loss 0.5463743209838867, val_loss 0.5322058796882629 \n","epoch 300, train_loss 0.5463743209838867, val_loss 0.5322058796882629 \n","    Record loss - saving at epoch 330\n","    epoch 330, train_loss 0.5433443784713745, val_loss 0.5321836471557617 \n","    Record loss - saving at epoch 348\n","    epoch 348, train_loss 0.5337421298027039, val_loss 0.5292742252349854 \n","epoch 350, train_loss 0.5290262699127197, val_loss 0.5500990748405457 \n","    Record loss - saving at epoch 362\n","    epoch 362, train_loss 0.549272358417511, val_loss 0.5277389883995056 \n","    Record loss - saving at epoch 374\n","    epoch 374, train_loss 0.5260539650917053, val_loss 0.5277194976806641 \n","epoch 400, train_loss 0.5498881936073303, val_loss 0.5526124238967896 \n","    Record loss - saving at epoch 427\n","    epoch 427, train_loss 0.5265807509422302, val_loss 0.5260037779808044 \n","    Record loss - saving at epoch 433\n","    epoch 433, train_loss 0.5287805795669556, val_loss 0.5213965177536011 \n","    Record loss - saving at epoch 439\n","    epoch 439, train_loss 0.5231675505638123, val_loss 0.5159497261047363 \n","epoch 450, train_loss 0.5273504257202148, val_loss 0.5461195111274719 \n","    Record loss - saving at epoch 493\n","    epoch 493, train_loss 0.45672091841697693, val_loss 0.5096037983894348 \n","    Record loss - saving at epoch 494\n","    epoch 494, train_loss 0.44467225670814514, val_loss 0.4873502552509308 \n","    Record loss - saving at epoch 495\n","    epoch 495, train_loss 0.44482797384262085, val_loss 0.48285698890686035 \n","    Record loss - saving at epoch 497\n","    epoch 497, train_loss 0.3839574456214905, val_loss 0.3558324873447418 \n","    Record loss - saving at epoch 499\n","    epoch 499, train_loss 0.34214282035827637, val_loss 0.33420562744140625 \n","epoch 500, train_loss 0.31889230012893677, val_loss 0.3396846354007721 \n","    Record loss - saving at epoch 501\n","    epoch 501, train_loss 0.3102928400039673, val_loss 0.32731741666793823 \n","    Record loss - saving at epoch 502\n","    epoch 502, train_loss 0.32412177324295044, val_loss 0.3041518032550812 \n","    Record loss - saving at epoch 504\n","    epoch 504, train_loss 0.3079863488674164, val_loss 0.2985879182815552 \n","    Record loss - saving at epoch 505\n","    epoch 505, train_loss 0.2991848289966583, val_loss 0.2955648601055145 \n","    Record loss - saving at epoch 507\n","    epoch 507, train_loss 0.30607089400291443, val_loss 0.2952403128147125 \n","    Record loss - saving at epoch 508\n","    epoch 508, train_loss 0.304127037525177, val_loss 0.2905507981777191 \n","    Record loss - saving at epoch 515\n","    epoch 515, train_loss 0.2943589389324188, val_loss 0.2892623543739319 \n","    Record loss - saving at epoch 517\n","    epoch 517, train_loss 0.2859659194946289, val_loss 0.28154870867729187 \n","epoch 550, train_loss 0.2772424519062042, val_loss 0.2908470034599304 \n","    Record loss - saving at epoch 562\n","    epoch 562, train_loss 0.28004807233810425, val_loss 0.2803393602371216 \n","epoch 600, train_loss 0.2597993016242981, val_loss 0.3070446848869324 \n","epoch 650, train_loss 0.2507084906101227, val_loss 0.32851317524909973 \n","    Record loss - saving at epoch 681\n","    epoch 681, train_loss 0.23006057739257812, val_loss 0.2785187363624573 \n","epoch 700, train_loss 0.25516706705093384, val_loss 0.31952571868896484 \n","    Record loss - saving at epoch 720\n","    epoch 720, train_loss 0.2270587980747223, val_loss 0.2598437964916229 \n","    Record loss - saving at epoch 725\n","    epoch 725, train_loss 0.22544434666633606, val_loss 0.24788907170295715 \n","    Record loss - saving at epoch 729\n","    epoch 729, train_loss 0.21776828169822693, val_loss 0.22190718352794647 \n","    Record loss - saving at epoch 734\n","    epoch 734, train_loss 0.20373444259166718, val_loss 0.21747098863124847 \n","    Record loss - saving at epoch 738\n","    epoch 738, train_loss 0.20031896233558655, val_loss 0.21705567836761475 \n","    Record loss - saving at epoch 740\n","    epoch 740, train_loss 0.19812819361686707, val_loss 0.19430549442768097 \n","    Record loss - saving at epoch 745\n","    epoch 745, train_loss 0.1805434376001358, val_loss 0.1792430430650711 \n","epoch 750, train_loss 0.1793370544910431, val_loss 0.18073616921901703 \n","    Record loss - saving at epoch 752\n","    epoch 752, train_loss 0.16816094517707825, val_loss 0.14378660917282104 \n","    Record loss - saving at epoch 757\n","    epoch 757, train_loss 0.16201382875442505, val_loss 0.1314978301525116 \n","    Record loss - saving at epoch 765\n","    epoch 765, train_loss 0.15464025735855103, val_loss 0.12813667953014374 \n","    Record loss - saving at epoch 767\n","    epoch 767, train_loss 0.16494902968406677, val_loss 0.11040905117988586 \n","    Record loss - saving at epoch 772\n","    epoch 772, train_loss 0.1455230563879013, val_loss 0.07800962775945663 \n","    Record loss - saving at epoch 785\n","    epoch 785, train_loss 0.1327032595872879, val_loss 0.06619194895029068 \n","    Record loss - saving at epoch 786\n","    epoch 786, train_loss 0.13343068957328796, val_loss 0.06552513688802719 \n","    Record loss - saving at epoch 790\n","    epoch 790, train_loss 0.12821243703365326, val_loss 0.06260539591312408 \n","    Record loss - saving at epoch 793\n","    epoch 793, train_loss 0.12962527573108673, val_loss 0.05346335098147392 \n","epoch 800, train_loss 0.12866459786891937, val_loss 0.06072823703289032 \n","    Record loss - saving at epoch 805\n","    epoch 805, train_loss 0.12481962144374847, val_loss 0.053422294557094574 \n","    Record loss - saving at epoch 809\n","    epoch 809, train_loss 0.1233157068490982, val_loss 0.050132982432842255 \n","    Record loss - saving at epoch 815\n","    epoch 815, train_loss 0.12926092743873596, val_loss 0.04931596294045448 \n","    Record loss - saving at epoch 829\n","    epoch 829, train_loss 0.11145254224538803, val_loss 0.04756788909435272 \n","    Record loss - saving at epoch 831\n","    epoch 831, train_loss 0.11448241770267487, val_loss 0.04671957716345787 \n","    Record loss - saving at epoch 836\n","    epoch 836, train_loss 0.11621876806020737, val_loss 0.04468487948179245 \n","epoch 850, train_loss 0.11234983056783676, val_loss 0.054928045719861984 \n","    Record loss - saving at epoch 851\n","    epoch 851, train_loss 0.1071702092885971, val_loss 0.04412277415394783 \n","    Record loss - saving at epoch 852\n","    epoch 852, train_loss 0.10825389623641968, val_loss 0.04086851328611374 \n","    Record loss - saving at epoch 886\n","    epoch 886, train_loss 0.09977379441261292, val_loss 0.04062496870756149 \n","epoch 900, train_loss 0.09889667481184006, val_loss 0.06238290295004845 \n","    Record loss - saving at epoch 920\n","    epoch 920, train_loss 0.1264885514974594, val_loss 0.03918944299221039 \n","    Record loss - saving at epoch 921\n","    epoch 921, train_loss 0.11701907962560654, val_loss 0.03675977513194084 \n","    Record loss - saving at epoch 922\n","    epoch 922, train_loss 0.11164546012878418, val_loss 0.03300515189766884 \n","epoch 950, train_loss 0.09271489828824997, val_loss 0.08883920311927795 \n","epoch 1000, train_loss 0.14769509434700012, val_loss 0.10215218365192413 \n","    Record loss - saving at epoch 1049\n","    epoch 1049, train_loss 0.11868923902511597, val_loss 0.03299751132726669 \n","epoch 1050, train_loss 0.11372736841440201, val_loss 0.06686999648809433 \n","    Record loss - saving at epoch 1054\n","    epoch 1054, train_loss 0.11555887758731842, val_loss 0.030203258618712425 \n","    Record loss - saving at epoch 1069\n","    epoch 1069, train_loss 0.11756684631109238, val_loss 0.028914442285895348 \n","    Record loss - saving at epoch 1071\n","    epoch 1071, train_loss 0.1107456237077713, val_loss 0.028751077130436897 \n","    Record loss - saving at epoch 1073\n","    epoch 1073, train_loss 0.11215444654226303, val_loss 0.027930542826652527 \n","    Record loss - saving at epoch 1099\n","    epoch 1099, train_loss 0.10228985548019409, val_loss 0.027194347232580185 \n","epoch 1100, train_loss 0.103876031935215, val_loss 0.031706761568784714 \n","    Record loss - saving at epoch 1105\n","    epoch 1105, train_loss 0.0997300073504448, val_loss 0.027192534878849983 \n","    Record loss - saving at epoch 1107\n","    epoch 1107, train_loss 0.10114525258541107, val_loss 0.027178745716810226 \n","    Record loss - saving at epoch 1124\n","    epoch 1124, train_loss 0.10179923474788666, val_loss 0.02705511823296547 \n","    Record loss - saving at epoch 1132\n","    epoch 1132, train_loss 0.0988881066441536, val_loss 0.022344602271914482 \n","epoch 1150, train_loss 0.11887264251708984, val_loss 0.059961624443531036 \n","epoch 1200, train_loss 0.11580514907836914, val_loss 0.02886616252362728 \n","    Record loss - saving at epoch 1216\n","    epoch 1216, train_loss 0.09594178199768066, val_loss 0.0222986601293087 \n","    Record loss - saving at epoch 1234\n","    epoch 1234, train_loss 0.08489004522562027, val_loss 0.021141184493899345 \n","epoch 1250, train_loss 0.08993358910083771, val_loss 0.08624608814716339 \n","    Record loss - saving at epoch 1268\n","    epoch 1268, train_loss 0.08352644741535187, val_loss 0.017233816906809807 \n","epoch 1300, train_loss 0.1063762679696083, val_loss 0.06400813162326813 \n","epoch 1350, train_loss 0.09963473677635193, val_loss 0.02269422635436058 \n","epoch 1400, train_loss 0.08941316604614258, val_loss 0.04563875496387482 \n","epoch 1450, train_loss 0.1300543248653412, val_loss 0.03186275437474251 \n","epoch 1500, train_loss 0.10092858225107193, val_loss 0.02386588789522648 \n","epoch 1550, train_loss 0.09178442507982254, val_loss 0.07013919949531555 \n","epoch 1600, train_loss 0.08326508849859238, val_loss 0.022497136145830154 \n","epoch 1650, train_loss 0.08352084457874298, val_loss 0.022903431206941605 \n","epoch 1700, train_loss 0.08760324120521545, val_loss 0.0240531787276268 \n","    Record loss - saving at epoch 1737\n","    epoch 1737, train_loss 0.07776359468698502, val_loss 0.017050467431545258 \n","epoch 1750, train_loss 0.08252459764480591, val_loss 0.02519880421459675 \n","epoch 1800, train_loss 0.08433464914560318, val_loss 0.02493772655725479 \n","epoch 1850, train_loss 0.07689639925956726, val_loss 0.020006800070405006 \n","    Record loss - saving at epoch 1858\n","    epoch 1858, train_loss 0.07530105859041214, val_loss 0.016097979620099068 \n","epoch 1900, train_loss 0.0850721225142479, val_loss 0.021949511021375656 \n","epoch 1950, train_loss 0.08047753572463989, val_loss 0.017351871356368065 \n","epoch 2000, train_loss 0.10129866749048233, val_loss 0.025609495118260384 \n","epoch 2050, train_loss 0.07987239956855774, val_loss 0.02166510745882988 \n","epoch 2100, train_loss 0.10674291849136353, val_loss 0.0512622632086277 \n","epoch 2150, train_loss 0.07289333641529083, val_loss 0.017514243721961975 \n","    Record loss - saving at epoch 2160\n","    epoch 2160, train_loss 0.07101615518331528, val_loss 0.01510685682296753 \n","epoch 2200, train_loss 0.08393934369087219, val_loss 0.022727012634277344 \n","    Record loss - saving at epoch 2206\n","    epoch 2206, train_loss 0.07042580842971802, val_loss 0.014897573739290237 \n","    Record loss - saving at epoch 2207\n","    epoch 2207, train_loss 0.06809601187705994, val_loss 0.01323351077735424 \n","epoch 2250, train_loss 0.07085131108760834, val_loss 0.023942599073052406 \n","epoch 2300, train_loss 0.08590901643037796, val_loss 0.029357867315411568 \n","epoch 2350, train_loss 0.06734758615493774, val_loss 0.014443754218518734 \n","epoch 2400, train_loss 0.07673259824514389, val_loss 0.021905483677983284 \n","epoch 2450, train_loss 0.06830017268657684, val_loss 0.024974964559078217 \n","epoch 2500, train_loss 0.06722148507833481, val_loss 0.01911582052707672 \n","epoch 2550, train_loss 0.07081835716962814, val_loss 0.017431680113077164 \n","epoch 2600, train_loss 0.08922796696424484, val_loss 0.02887783944606781 \n","epoch 2650, train_loss 0.08489281684160233, val_loss 0.02955174632370472 \n","epoch 2700, train_loss 0.7594977021217346, val_loss 0.7620728611946106 \n","max patience reached, stopping training\n"]}],"source":["# training loop\n","lowest_val_loss = 0\n","best_loss = False\n","\n","max_patience = 500\n","curr_patience = max_patience\n","\n","# datetime object containing current date and time\n","best_epoch = 0\n","now = datetime.now()\n","dt_string = now.strftime(f\"models/{model.model_type}_%d-%m-%Y_%H-%M-%S\")\n","\n","os.mkdir(os.path.join(\".\", dt_string))\n","\n","for epoch in range(max_epochs):\n","    #ep_loss = myk_train.train_epoch_interval(model, train_dl, loss_functions, optimiser, device=device)\n","    ep_loss = myk_train.train_epoch_interval(model, train_dl, loss_functions, optimiser, device=device)\n","\n","    #ep_loss = myk_train.train_epoch(model, train_dl, loss_functions, optimiser, device=device)\n","    val_loss = myk_train.compute_batch_loss(model, val_dl, loss_functions, device=device)\n","    writer.add_scalar(\"Loss/val\", val_loss, epoch)\n","    writer.add_scalar(\"Loss/train\", ep_loss, epoch)\n","\n","    # check if we have beaten our best loss to date\n","    if lowest_val_loss == 0:# first run\n","        lowest_val_loss = val_loss\n","    elif val_loss < lowest_val_loss:# new record\n","        lowest_val_loss = val_loss\n","        best_loss = True\n","    else: # no improvement\n","        best_loss = False\n","        curr_patience -= 1\n","\n","    if best_loss: # save best model so far\n","        best_epoch = epoch\n","        print(f\"    Record loss - saving at epoch {epoch}\")\n","        # save for RTNeural\n","        model.save_for_rtneural(f\"{dt_string}/model.json\")\n","        # save for pythorch\n","        torch.save(model.state_dict(), f\"{dt_string}/model.ph\")\n","        print(f\"    epoch {epoch}, train_loss {ep_loss}, val_loss {val_loss} \")\n","        curr_patience = max_patience\n","    if epoch % 50 == 0: # save an example processed audio file\n","        myk_evaluate.run_file_through_model(model, test_file, audio_folder + \"/\" + run_name + str(epoch)+\".wav\")\n","        print(f\"epoch {epoch}, train_loss {ep_loss}, val_loss {val_loss} \")\n","    if curr_patience == 0:\n","        print(\"max patience reached, stopping training\")\n","        # load best parameters in the model\n","        model.load_state_dict(torch.load(f\"{dt_string}/model.ph\"))\n","        model.eval() # set inference state in the possible layers that need it\n","        myk_evaluate.run_file_through_model(model, test_file, audio_folder + \"/\" + run_name + str(best_epoch)+\"_BEST.wav\")\n","        break"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":249,"status":"ok","timestamp":1688827516389,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"edQeVKnKqIlY"},"outputs":[],"source":["to_append = [f'\\n{run_name}/*.wav', f'\\n!{run_name}/*_BEST.wav']\n","\n","with open(f'{audio_folder}/../.gitignore', \"r+\") as gitignore:\n","    read = gitignore.read()\n","    if to_append[0][2:len(to_append[0])] not in read:\n","        gitignore.writelines(to_append)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1R90W11c0RiDl3RfZT-sX8pRNVVVbmSYh","timestamp":1685016693193}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":0}