{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1445,"status":"ok","timestamp":1688917292658,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"AUk8TQCYzAAe","outputId":"d452f5ba-e0d1-430d-af70-334403a50d09"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/Othercomputers/Il mio MacBook Pro/MagicKnob\n","/content/drive/Othercomputers/Il mio MacBook Pro/MagicKnob/python\n"]}],"source":["import os\n","from datetime import datetime\n","\n","drive = True\n","\n","if drive:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    %cd /content/drive/Othercomputers/Il mio MacBook Pro/MagicKnob\n","\n","    path = \"/content/drive/Othercomputers/Il mio MacBook Pro/MagicKnob/\"\n","\n","    # check python file folder\n","    assert os.path.exists(path + \"python\"), f\"Upload python files in {path}python\"\n","    %cd ./python\n","\n","    # check data folder\n","    assert os.path.exists(path + \"data\"), f\"Upload data files in {path}data\"\n","else:\n","    path = \"../\"\n","\n","    # check python file folder\n","    assert os.path.exists(path + \"python\"), f\"Upload python files in {path}python\"\n","\n","    # check data folder\n","    assert os.path.exists(path + \"data\"), f\"Upload data files in {path}data\""]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2062,"status":"ok","timestamp":1688917294718,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"Wgz6WkLZzHDM"},"outputs":[],"source":["import myk_data\n","import myk_models\n","import myk_loss\n","import myk_train\n","import myk_evaluate\n","\n","import torch\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5870,"status":"ok","timestamp":1688917300584,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"mX_cQAvgzMLR"},"outputs":[],"source":["from torch.utils.tensorboard import SummaryWriter"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1688917300585,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"FXrHBuv6zQPg"},"outputs":[],"source":["# used for the writing of example outputs\n","run_name=\"audio_logic_dist\"\n","# dataset : need an input and output folder in this folder\n","audio_folder = f\"../data/{run_name}\"\n","#audio_folder = \"../../data/audio_ht1\"\n","assert os.path.exists(audio_folder), f\"Audio folder  not found. Looked for {audio_folder}\"\n","# used to render example output during training\n","test_file = \"../data/guitar.wav\"\n","assert os.path.exists(test_file), \"Test file not found. Looked for \" + test_file"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1688917300585,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"wJPGiuLV0ylP"},"outputs":[],"source":["# initialize net specs\n","lstm_hidden_size = 32\n","learning_rate = 5e-3\n","batch_size = 50\n","max_epochs = 10000\n","\n","# create the logger for tensorboard\n","writer = SummaryWriter()"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":714,"status":"ok","timestamp":1688917301282,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"izzFWaHj0zej","outputId":"ce7be031-3a27-4161-b4ad-01b385f89d0b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading dataset from folder  ../data/audio_logic_dist\n","loading input and output of logicdist\n","    loading output of logicdist\n","    generate_dataset:: Loaded frames from audio file 22050\n","    found input fragments of shape (120, 22050, 1)\n","    found output fragments of shape (120, 22050, 1)\n","    total input shape: (120, 22050, 1)\n","    total output shape: (120, 22050, 1)\n","\n","total input tensor shape torch.Size([120, 22050, 1])\n","total output tensor shape torch.Size([120, 22050, 1])\n","Splitting dataset\n"]}],"source":["print(\"Loading dataset from folder \", audio_folder)\n","dataset = myk_data.generate_dataset(audio_folder + \"/input/\", audio_folder + \"/output/\", frag_len_seconds=0.5)\n","\n","print(\"Splitting dataset\")\n","train_ds, val_ds, test_ds = myk_data.get_train_valid_test_datasets(dataset)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1688917301282,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"p5wnmGGzU1KX","outputId":"c59eab52-dd83-425d-fc86-78d03ca50865"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[-0.0791],\n","         [-0.0757],\n","         [-0.0721],\n","         ...,\n","         [ 0.0123],\n","         [ 0.0109],\n","         [ 0.0096]]),\n"," tensor([[-0.9982],\n","         [-0.9958],\n","         [-0.9918],\n","         ...,\n","         [ 0.3502],\n","         [ 0.3386],\n","         [ 0.3236]]))"]},"metadata":{},"execution_count":7}],"source":["dataset[70]"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1688917301282,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"KpY8DxYl1Cb0","outputId":"d2085682-dc74-42ce-a65e-fda8b0a98dae"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda device available\n"]}],"source":["# test GPU, must be done after splitting\n","device = myk_train.get_device()"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1688917301283,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"D3WztACX2UOd"},"outputs":[],"source":["# create data loaders\n","train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=device))\n","val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=device))\n","test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=device))"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4267,"status":"ok","timestamp":1688917305547,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"8JZL-Qqa2ORT","outputId":"0bc9145f-2288-4569-d8df-79c5db84c3e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["SimpleLSTM(\n","  (lstm): LSTM(1, 32, batch_first=True)\n","  (dense): Linear(in_features=32, out_features=1, bias=True)\n",")\n"]}],"source":["model = myk_models.SimpleLSTM(hidden_size=lstm_hidden_size).to(device)\n","print(model)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1688917305548,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"4CuDKuR92cxv"},"outputs":[],"source":["# crate optimizer and loss function\n","optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n","#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', factor=0.5, patience=5, verbose=True) non lo usava\n","\n","loss_functions = myk_loss.LossWrapper()\n","\n","# https://github.com/Alec-Wright/Automated-GuitarAmpModelling/blob/main/dist_model_recnet.py\n","# https://github.com/Alec-Wright/CoreAudioML/blob/bad9469f94a2fa63a50d70ff75f5eff2208ba03f/training.py"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1688917305549,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"-6cmEvsqHwL7"},"outputs":[],"source":["#%load_ext tensorboard\n","#%tensorboard --logdir logs"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":456544,"status":"ok","timestamp":1688917762074,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"L2_vgpXU2tvW","outputId":"8c8d0d24-8ca9-461d-cf02-666bb8879c80"},"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 0, train_loss 0.7477530837059021, val_loss 0.723615288734436 \n","    Record loss - saving at epoch 1\n","    epoch 1, train_loss 0.5388651490211487, val_loss 0.3098590075969696 \n","    Record loss - saving at epoch 2\n","    epoch 2, train_loss 0.3124450445175171, val_loss 0.1931963711977005 \n","    Record loss - saving at epoch 3\n","    epoch 3, train_loss 0.1267620325088501, val_loss 0.10071612894535065 \n","    Record loss - saving at epoch 4\n","    epoch 4, train_loss 0.08135445415973663, val_loss 0.06868735700845718 \n","    Record loss - saving at epoch 5\n","    epoch 5, train_loss 0.06168042868375778, val_loss 0.05555841699242592 \n","    Record loss - saving at epoch 6\n","    epoch 6, train_loss 0.048605337738990784, val_loss 0.04489579051733017 \n","    Record loss - saving at epoch 7\n","    epoch 7, train_loss 0.03794672340154648, val_loss 0.036930810660123825 \n","    Record loss - saving at epoch 8\n","    epoch 8, train_loss 0.026801547035574913, val_loss 0.02317764051258564 \n","    Record loss - saving at epoch 11\n","    epoch 11, train_loss 0.015113845467567444, val_loss 0.017631126567721367 \n","    Record loss - saving at epoch 12\n","    epoch 12, train_loss 0.012552904896438122, val_loss 0.013688962906599045 \n","    Record loss - saving at epoch 13\n","    epoch 13, train_loss 0.010504570789635181, val_loss 0.01221384759992361 \n","    Record loss - saving at epoch 14\n","    epoch 14, train_loss 0.00956529937684536, val_loss 0.010725883767008781 \n","    Record loss - saving at epoch 15\n","    epoch 15, train_loss 0.008558724075555801, val_loss 0.009807266294956207 \n","    Record loss - saving at epoch 16\n","    epoch 16, train_loss 0.007927956059575081, val_loss 0.008923064917325974 \n","    Record loss - saving at epoch 17\n","    epoch 17, train_loss 0.007242856081575155, val_loss 0.008028448559343815 \n","    Record loss - saving at epoch 18\n","    epoch 18, train_loss 0.0067139193415641785, val_loss 0.007369621656835079 \n","    Record loss - saving at epoch 19\n","    epoch 19, train_loss 0.00639443239197135, val_loss 0.006997046992182732 \n","    Record loss - saving at epoch 20\n","    epoch 20, train_loss 0.005869972053915262, val_loss 0.006356741301715374 \n","    Record loss - saving at epoch 23\n","    epoch 23, train_loss 0.006271840538829565, val_loss 0.006337307393550873 \n","    Record loss - saving at epoch 24\n","    epoch 24, train_loss 0.005379881244152784, val_loss 0.005974647123366594 \n","    Record loss - saving at epoch 25\n","    epoch 25, train_loss 0.0050304969772696495, val_loss 0.005682218819856644 \n","    Record loss - saving at epoch 26\n","    epoch 26, train_loss 0.004957837052643299, val_loss 0.00544391106814146 \n","    Record loss - saving at epoch 27\n","    epoch 27, train_loss 0.004648714791983366, val_loss 0.00532452343031764 \n","    Record loss - saving at epoch 28\n","    epoch 28, train_loss 0.004570022225379944, val_loss 0.005137959960848093 \n","    Record loss - saving at epoch 29\n","    epoch 29, train_loss 0.004483709577471018, val_loss 0.005004054866731167 \n","    Record loss - saving at epoch 30\n","    epoch 30, train_loss 0.004347615875303745, val_loss 0.004840852227061987 \n","    Record loss - saving at epoch 31\n","    epoch 31, train_loss 0.004305352456867695, val_loss 0.004729119595140219 \n","    Record loss - saving at epoch 33\n","    epoch 33, train_loss 0.004171209409832954, val_loss 0.004574875347316265 \n","    Record loss - saving at epoch 37\n","    epoch 37, train_loss 0.004331373143941164, val_loss 0.004512266721576452 \n","    Record loss - saving at epoch 38\n","    epoch 38, train_loss 0.004032372497022152, val_loss 0.004350350704044104 \n","    Record loss - saving at epoch 39\n","    epoch 39, train_loss 0.0038654531817883253, val_loss 0.004257021937519312 \n","    Record loss - saving at epoch 40\n","    epoch 40, train_loss 0.003825691295787692, val_loss 0.004125298000872135 \n","    Record loss - saving at epoch 41\n","    epoch 41, train_loss 0.0037380110006779432, val_loss 0.00402189651504159 \n","    Record loss - saving at epoch 42\n","    epoch 42, train_loss 0.0036997131537646055, val_loss 0.00396615918725729 \n","    Record loss - saving at epoch 43\n","    epoch 43, train_loss 0.003649648977443576, val_loss 0.0038789405953139067 \n","    Record loss - saving at epoch 44\n","    epoch 44, train_loss 0.0036187577061355114, val_loss 0.0038019095081835985 \n","    Record loss - saving at epoch 45\n","    epoch 45, train_loss 0.003657899098470807, val_loss 0.0037539838813245296 \n","    Record loss - saving at epoch 46\n","    epoch 46, train_loss 0.003531955648213625, val_loss 0.0036510704085230827 \n","    Record loss - saving at epoch 47\n","    epoch 47, train_loss 0.003485454712063074, val_loss 0.0036054174415767193 \n","    Record loss - saving at epoch 48\n","    epoch 48, train_loss 0.0035295120906084776, val_loss 0.003589567029848695 \n","epoch 50, train_loss 0.006592323072254658, val_loss 0.009009495377540588 \n","    Record loss - saving at epoch 52\n","    epoch 52, train_loss 0.0035894415341317654, val_loss 0.0034072718117386103 \n","    Record loss - saving at epoch 53\n","    epoch 53, train_loss 0.0032852650620043278, val_loss 0.003248919267207384 \n","    Record loss - saving at epoch 54\n","    epoch 54, train_loss 0.0032731476239860058, val_loss 0.0032018108759075403 \n","    Record loss - saving at epoch 55\n","    epoch 55, train_loss 0.0031588382553309202, val_loss 0.0030906852334737778 \n","    Record loss - saving at epoch 56\n","    epoch 56, train_loss 0.003156078979372978, val_loss 0.00304563925601542 \n","    Record loss - saving at epoch 57\n","    epoch 57, train_loss 0.003074332606047392, val_loss 0.002968053799122572 \n","    Record loss - saving at epoch 58\n","    epoch 58, train_loss 0.00304565136320889, val_loss 0.002889063209295273 \n","    Record loss - saving at epoch 59\n","    epoch 59, train_loss 0.0030015096999704838, val_loss 0.002831523772329092 \n","    Record loss - saving at epoch 61\n","    epoch 61, train_loss 0.003175925463438034, val_loss 0.0027821653056889772 \n","    Record loss - saving at epoch 62\n","    epoch 62, train_loss 0.0029833742883056402, val_loss 0.0026459735818207264 \n","    Record loss - saving at epoch 64\n","    epoch 64, train_loss 0.002905752044171095, val_loss 0.0025509349070489407 \n","    Record loss - saving at epoch 65\n","    epoch 65, train_loss 0.0027934906538575888, val_loss 0.0024654469452798367 \n","    Record loss - saving at epoch 66\n","    epoch 66, train_loss 0.002807365031912923, val_loss 0.0024651153944432735 \n","    Record loss - saving at epoch 73\n","    epoch 73, train_loss 0.0030043558217585087, val_loss 0.00222731102257967 \n","    Record loss - saving at epoch 74\n","    epoch 74, train_loss 0.0025248064193874598, val_loss 0.002081046113744378 \n","    Record loss - saving at epoch 75\n","    epoch 75, train_loss 0.0024078518617898226, val_loss 0.002024297835305333 \n","    Record loss - saving at epoch 76\n","    epoch 76, train_loss 0.002363379579037428, val_loss 0.0018938437569886446 \n","    Record loss - saving at epoch 78\n","    epoch 78, train_loss 0.002258066786453128, val_loss 0.0018161447951570153 \n","    Record loss - saving at epoch 80\n","    epoch 80, train_loss 0.0022392463870346546, val_loss 0.0017217905260622501 \n","    Record loss - saving at epoch 82\n","    epoch 82, train_loss 0.002200386021286249, val_loss 0.0016461118357256055 \n","    Record loss - saving at epoch 83\n","    epoch 83, train_loss 0.002153004053980112, val_loss 0.0016098810592666268 \n","    Record loss - saving at epoch 84\n","    epoch 84, train_loss 0.002151984954252839, val_loss 0.001594061148352921 \n","    Record loss - saving at epoch 85\n","    epoch 85, train_loss 0.002132294001057744, val_loss 0.0015473873354494572 \n","    Record loss - saving at epoch 86\n","    epoch 86, train_loss 0.002075634431093931, val_loss 0.0014967528404667974 \n","    Record loss - saving at epoch 87\n","    epoch 87, train_loss 0.002063098130747676, val_loss 0.0014784776140004396 \n","    Record loss - saving at epoch 88\n","    epoch 88, train_loss 0.002062672981992364, val_loss 0.0014767642132937908 \n","    Record loss - saving at epoch 89\n","    epoch 89, train_loss 0.00200605858117342, val_loss 0.001428005052730441 \n","    Record loss - saving at epoch 90\n","    epoch 90, train_loss 0.0019915287848562002, val_loss 0.001417901017703116 \n","    Record loss - saving at epoch 91\n","    epoch 91, train_loss 0.0020019563380628824, val_loss 0.0014105387963354588 \n","    Record loss - saving at epoch 92\n","    epoch 92, train_loss 0.001992868259549141, val_loss 0.0013901033671572804 \n","    Record loss - saving at epoch 93\n","    epoch 93, train_loss 0.001977579202502966, val_loss 0.0013625890715047717 \n","    Record loss - saving at epoch 94\n","    epoch 94, train_loss 0.001948869088664651, val_loss 0.0013433514395728707 \n","    Record loss - saving at epoch 95\n","    epoch 95, train_loss 0.0019326157635077834, val_loss 0.001330940518528223 \n","epoch 100, train_loss 0.0053938888013362885, val_loss 0.002189927501603961 \n","    Record loss - saving at epoch 102\n","    epoch 102, train_loss 0.001914679422043264, val_loss 0.0012173543218523264 \n","    Record loss - saving at epoch 103\n","    epoch 103, train_loss 0.001820010133087635, val_loss 0.0011689015664160252 \n","    Record loss - saving at epoch 104\n","    epoch 104, train_loss 0.0017829241696745157, val_loss 0.0011528658214956522 \n","    Record loss - saving at epoch 105\n","    epoch 105, train_loss 0.0017787717515602708, val_loss 0.0011360440403223038 \n","    Record loss - saving at epoch 106\n","    epoch 106, train_loss 0.001772242016158998, val_loss 0.0011161884758621454 \n","    Record loss - saving at epoch 108\n","    epoch 108, train_loss 0.0017840589862316847, val_loss 0.0011075349757447839 \n","    Record loss - saving at epoch 109\n","    epoch 109, train_loss 0.0017344517400488257, val_loss 0.001093837432563305 \n","    Record loss - saving at epoch 111\n","    epoch 111, train_loss 0.0017173008527606726, val_loss 0.0010766962077468634 \n","    Record loss - saving at epoch 112\n","    epoch 112, train_loss 0.0017191693186759949, val_loss 0.0010701768333092332 \n","    Record loss - saving at epoch 113\n","    epoch 113, train_loss 0.0017093619098886847, val_loss 0.0010647210292518139 \n","    Record loss - saving at epoch 116\n","    epoch 116, train_loss 0.0016771985683590174, val_loss 0.0010425376240164042 \n","    Record loss - saving at epoch 118\n","    epoch 118, train_loss 0.001639383495785296, val_loss 0.0010058879852294922 \n","    Record loss - saving at epoch 121\n","    epoch 121, train_loss 0.0016109332209452987, val_loss 0.0009630938293412328 \n","    Record loss - saving at epoch 128\n","    epoch 128, train_loss 0.0016066586831584573, val_loss 0.000927278830204159 \n","    Record loss - saving at epoch 130\n","    epoch 130, train_loss 0.0015517587307840586, val_loss 0.0008954481454566121 \n","    Record loss - saving at epoch 131\n","    epoch 131, train_loss 0.0015485226176679134, val_loss 0.0008861675160005689 \n","    Record loss - saving at epoch 132\n","    epoch 132, train_loss 0.0015518213622272015, val_loss 0.0008852300816215575 \n","    Record loss - saving at epoch 133\n","    epoch 133, train_loss 0.001531069865450263, val_loss 0.0008835961343720555 \n","    Record loss - saving at epoch 135\n","    epoch 135, train_loss 0.0015309186419472098, val_loss 0.0008791395812295377 \n","    Record loss - saving at epoch 136\n","    epoch 136, train_loss 0.0015294940676540136, val_loss 0.0008656845311634243 \n","    Record loss - saving at epoch 144\n","    epoch 144, train_loss 0.0014731342671439052, val_loss 0.0008541002753190696 \n","    Record loss - saving at epoch 147\n","    epoch 147, train_loss 0.001475163153372705, val_loss 0.0008514677756465971 \n","    Record loss - saving at epoch 148\n","    epoch 148, train_loss 0.0014447750290855765, val_loss 0.0008421776001341641 \n","epoch 150, train_loss 0.001479935715906322, val_loss 0.0009231510339304805 \n","    Record loss - saving at epoch 156\n","    epoch 156, train_loss 0.0014718727907165885, val_loss 0.0008284199284389615 \n","    Record loss - saving at epoch 162\n","    epoch 162, train_loss 0.0014294523280113935, val_loss 0.0008071411866694689 \n","    Record loss - saving at epoch 165\n","    epoch 165, train_loss 0.0014069619355723262, val_loss 0.0007910709246061742 \n","    Record loss - saving at epoch 166\n","    epoch 166, train_loss 0.0013879232574254274, val_loss 0.0007822541519999504 \n","epoch 200, train_loss 0.001422788598574698, val_loss 0.0008251076214946806 \n","epoch 250, train_loss 0.0014128783950582147, val_loss 0.0008115590317174792 \n","epoch 300, train_loss 0.0013507271651178598, val_loss 0.0007839564932510257 \n","    Record loss - saving at epoch 320\n","    epoch 320, train_loss 0.0013624459970742464, val_loss 0.0007786675705574453 \n","    Record loss - saving at epoch 330\n","    epoch 330, train_loss 0.0013438130263239145, val_loss 0.0007757126586511731 \n","epoch 350, train_loss 0.00140157132409513, val_loss 0.0008179352735169232 \n","    Record loss - saving at epoch 369\n","    epoch 369, train_loss 0.0013150358572602272, val_loss 0.0007734251557849348 \n","    Record loss - saving at epoch 370\n","    epoch 370, train_loss 0.0013433300191536546, val_loss 0.000771787716075778 \n","    Record loss - saving at epoch 374\n","    epoch 374, train_loss 0.0013212276389822364, val_loss 0.0007687379838898778 \n","    Record loss - saving at epoch 383\n","    epoch 383, train_loss 0.0013186575379222631, val_loss 0.0007678275578655303 \n","    Record loss - saving at epoch 392\n","    epoch 392, train_loss 0.001331936800852418, val_loss 0.0007606509607285261 \n","epoch 400, train_loss 0.0013859427999705076, val_loss 0.0008441975805908442 \n","    Record loss - saving at epoch 424\n","    epoch 424, train_loss 0.0013065197272226214, val_loss 0.0007531362352892756 \n","epoch 450, train_loss 0.0013241387205198407, val_loss 0.0008347135735675693 \n","    Record loss - saving at epoch 457\n","    epoch 457, train_loss 0.0012539265444502234, val_loss 0.0007502518710680306 \n","    Record loss - saving at epoch 484\n","    epoch 484, train_loss 0.0012750366004183888, val_loss 0.0007461998029612005 \n","    Record loss - saving at epoch 488\n","    epoch 488, train_loss 0.0012266443809494376, val_loss 0.0007385328644886613 \n","epoch 500, train_loss 0.0012557251611724496, val_loss 0.0010061148786917329 \n","    Record loss - saving at epoch 507\n","    epoch 507, train_loss 0.0012119601015001535, val_loss 0.0007312064990401268 \n","    Record loss - saving at epoch 511\n","    epoch 511, train_loss 0.0012786833103746176, val_loss 0.0007300881552509964 \n","    Record loss - saving at epoch 518\n","    epoch 518, train_loss 0.0012107327347621322, val_loss 0.0007206868613138795 \n","    Record loss - saving at epoch 541\n","    epoch 541, train_loss 0.001230105059221387, val_loss 0.0007186551229096949 \n","    Record loss - saving at epoch 544\n","    epoch 544, train_loss 0.0011956084053963423, val_loss 0.0007111029117368162 \n","    Record loss - saving at epoch 545\n","    epoch 545, train_loss 0.0012001529103145003, val_loss 0.0007059074123390019 \n","    Record loss - saving at epoch 546\n","    epoch 546, train_loss 0.0011871118331328034, val_loss 0.0007025613449513912 \n","epoch 550, train_loss 0.0012140882899984717, val_loss 0.0008196443668566644 \n","    Record loss - saving at epoch 551\n","    epoch 551, train_loss 0.0012005522148683667, val_loss 0.0006924470071680844 \n","    Record loss - saving at epoch 568\n","    epoch 568, train_loss 0.0011479367967694998, val_loss 0.0006922487518750131 \n","    Record loss - saving at epoch 569\n","    epoch 569, train_loss 0.0011789436684921384, val_loss 0.0006917034625075758 \n","    Record loss - saving at epoch 590\n","    epoch 590, train_loss 0.0011035510106012225, val_loss 0.0006862267036922276 \n","    Record loss - saving at epoch 591\n","    epoch 591, train_loss 0.0011128154583275318, val_loss 0.000683955498971045 \n","epoch 600, train_loss 0.0011537112295627594, val_loss 0.0007119794609025121 \n","    Record loss - saving at epoch 613\n","    epoch 613, train_loss 0.0011094134533777833, val_loss 0.0006835036328993738 \n","    Record loss - saving at epoch 619\n","    epoch 619, train_loss 0.0011081707198172808, val_loss 0.0006785221048630774 \n","    Record loss - saving at epoch 642\n","    epoch 642, train_loss 0.001094345236197114, val_loss 0.0006763123092241585 \n","    Record loss - saving at epoch 647\n","    epoch 647, train_loss 0.0010646478040143847, val_loss 0.000666523992549628 \n","epoch 650, train_loss 0.001306787016801536, val_loss 0.0007966167177073658 \n","    Record loss - saving at epoch 655\n","    epoch 655, train_loss 0.0010828394442796707, val_loss 0.0006621237844228745 \n","epoch 700, train_loss 0.001100198714993894, val_loss 0.0007271539070643485 \n","epoch 750, train_loss 0.0010916332248598337, val_loss 0.0007129519362933934 \n","    Record loss - saving at epoch 760\n","    epoch 760, train_loss 0.0010615407954901457, val_loss 0.0006601838394999504 \n","    Record loss - saving at epoch 767\n","    epoch 767, train_loss 0.0010544548276811838, val_loss 0.0006570102996192873 \n","    Record loss - saving at epoch 792\n","    epoch 792, train_loss 0.0010681917192414403, val_loss 0.0006531217368319631 \n","    Record loss - saving at epoch 794\n","    epoch 794, train_loss 0.0010603915434330702, val_loss 0.0006529731908813119 \n","    Record loss - saving at epoch 795\n","    epoch 795, train_loss 0.001044906210154295, val_loss 0.000652432965580374 \n","    Record loss - saving at epoch 798\n","    epoch 798, train_loss 0.0010413062991574407, val_loss 0.0006440366269089282 \n","epoch 800, train_loss 0.0010403201449662447, val_loss 0.0007458271575160325 \n","epoch 850, train_loss 0.0010841129114851356, val_loss 0.0006645333487540483 \n","    Record loss - saving at epoch 879\n","    epoch 879, train_loss 0.0010255554225295782, val_loss 0.0006433413363993168 \n","epoch 900, train_loss 0.0010943536181002855, val_loss 0.0007190472679212689 \n","    Record loss - saving at epoch 928\n","    epoch 928, train_loss 0.0010469933040440083, val_loss 0.000642915372736752 \n","    Record loss - saving at epoch 947\n","    epoch 947, train_loss 0.001031318330205977, val_loss 0.0006379810511134565 \n","    Record loss - saving at epoch 949\n","    epoch 949, train_loss 0.001060976879671216, val_loss 0.0006379326805472374 \n","epoch 950, train_loss 0.0010370365343987942, val_loss 0.0006761201075278223 \n","    Record loss - saving at epoch 975\n","    epoch 975, train_loss 0.0012205045204609632, val_loss 0.000635334406979382 \n","epoch 1000, train_loss 0.001019133604131639, val_loss 0.0006466402555815876 \n","    Record loss - saving at epoch 1029\n","    epoch 1029, train_loss 0.001094305538572371, val_loss 0.0006351437186822295 \n","    Record loss - saving at epoch 1030\n","    epoch 1030, train_loss 0.0010221160482615232, val_loss 0.0006222014781087637 \n","epoch 1050, train_loss 0.0010771770030260086, val_loss 0.0007081638323143125 \n","epoch 1100, train_loss 0.0013715166132897139, val_loss 0.0008104878361336887 \n","epoch 1150, train_loss 0.0014270029496401548, val_loss 0.0009206131799146533 \n","epoch 1200, train_loss 0.001056994078680873, val_loss 0.0006407255423255265 \n","epoch 1250, train_loss 0.0010296936379745603, val_loss 0.0006463159224949777 \n","    Record loss - saving at epoch 1262\n","    epoch 1262, train_loss 0.001005007652565837, val_loss 0.0006206710822880268 \n","epoch 1300, train_loss 0.0010929516283795238, val_loss 0.0006710760062560439 \n","    Record loss - saving at epoch 1326\n","    epoch 1326, train_loss 0.0010028776014223695, val_loss 0.000619548256509006 \n","    Record loss - saving at epoch 1349\n","    epoch 1349, train_loss 0.0010057836771011353, val_loss 0.0006191349239088595 \n","    Record loss - saving at epoch 1350\n","    epoch 1350, train_loss 0.0010030760895460844, val_loss 0.000613233947660774 \n","epoch 1350, train_loss 0.0010030760895460844, val_loss 0.000613233947660774 \n","epoch 1400, train_loss 0.0010095887118950486, val_loss 0.0006271663587540388 \n","    Record loss - saving at epoch 1439\n","    epoch 1439, train_loss 0.0009983920026570559, val_loss 0.0006132290000095963 \n","    Record loss - saving at epoch 1440\n","    epoch 1440, train_loss 0.0009976672008633614, val_loss 0.0006096360157243907 \n","    Record loss - saving at epoch 1441\n","    epoch 1441, train_loss 0.0009943477343767881, val_loss 0.0006083154585212469 \n","epoch 1450, train_loss 0.001011264743283391, val_loss 0.0007246860768646002 \n","epoch 1500, train_loss 0.0010089160641655326, val_loss 0.0006296780193224549 \n","epoch 1550, train_loss 0.0010099336504936218, val_loss 0.0006250898586586118 \n","epoch 1600, train_loss 0.0027910321950912476, val_loss 0.001314595341682434 \n","epoch 1650, train_loss 0.0011465862626209855, val_loss 0.0007515737088397145 \n","epoch 1700, train_loss 0.0010186797007918358, val_loss 0.0006602826179005206 \n","epoch 1750, train_loss 0.001094328355975449, val_loss 0.0006277408101595938 \n","epoch 1800, train_loss 0.0009898284915834665, val_loss 0.000634319381788373 \n","epoch 1850, train_loss 0.0010044958908110857, val_loss 0.0006586985546164215 \n","epoch 1900, train_loss 0.0010443383362144232, val_loss 0.000638458994217217 \n","max patience reached, stopping training\n"]}],"source":["# training loop\n","lowest_val_loss = 0\n","best_loss = False\n","\n","max_patience = 500\n","curr_patience = max_patience\n","\n","# datetime object containing current date and time\n","best_epoch = 0\n","now = datetime.now()\n","dt_string = now.strftime(f\"models/{model.model_type}_%d-%m-%Y_%H-%M-%S\")\n","\n","os.mkdir(os.path.join(\".\", dt_string))\n","\n","for epoch in range(max_epochs):\n","    #ep_loss = myk_train.train_epoch_interval(model, train_dl, loss_functions, optimiser, device=device)\n","    ep_loss = myk_train.train_epoch_interval(model, train_dl, loss_functions, optimiser, device=device)\n","\n","    #ep_loss = myk_train.train_epoch(model, train_dl, loss_functions, optimiser, device=device)\n","    val_loss = myk_train.compute_batch_loss(model, val_dl, loss_functions, device=device)\n","    writer.add_scalar(\"Loss/val\", val_loss, epoch)\n","    writer.add_scalar(\"Loss/train\", ep_loss, epoch)\n","\n","    # check if we have beaten our best loss to date\n","    if lowest_val_loss == 0:# first run\n","        lowest_val_loss = val_loss\n","    elif val_loss < lowest_val_loss:# new record\n","        lowest_val_loss = val_loss\n","        best_loss = True\n","    else: # no improvement\n","        best_loss = False\n","        curr_patience -= 1\n","\n","    if best_loss: # save best model so far\n","        best_epoch = epoch\n","        print(f\"    Record loss - saving at epoch {epoch}\")\n","        # save for RTNeural\n","        model.save_for_rtneural(f\"{dt_string}/model.json\")\n","        # save for pythorch\n","        torch.save(model.state_dict(), f\"{dt_string}/model.ph\")\n","        print(f\"    epoch {epoch}, train_loss {ep_loss}, val_loss {val_loss} \")\n","        curr_patience = max_patience\n","    if epoch % 50 == 0: # save an example processed audio file\n","        myk_evaluate.run_file_through_model(model, test_file, audio_folder + \"/\" + run_name + str(epoch)+\".wav\")\n","        print(f\"epoch {epoch}, train_loss {ep_loss}, val_loss {val_loss} \")\n","    if curr_patience == 0:\n","        print(\"max patience reached, stopping training\")\n","        # load best parameters in the model\n","        model.load_state_dict(torch.load(f\"{dt_string}/model.ph\"))\n","        model.eval() # set inference state in the possible layers that need it\n","        myk_evaluate.run_file_through_model(model, test_file, audio_folder + \"/\" + run_name + str(best_epoch)+\"_BEST.wav\")\n","        break"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":187,"status":"ok","timestamp":1688917766544,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"edQeVKnKqIlY"},"outputs":[],"source":["to_append = [f'\\n{run_name}/*.wav', f'\\n!{run_name}/*_BEST*.wav']\n","\n","with open(f'{audio_folder}/../.gitignore', \"r+\") as gitignore:\n","    read = gitignore.read()\n","    if to_append[0][2:len(to_append[0])] not in read:\n","        gitignore.writelines(to_append)"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1688917762075,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"1dhW_Q3_DtSN"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1R90W11c0RiDl3RfZT-sX8pRNVVVbmSYh","timestamp":1685016693193}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":0}