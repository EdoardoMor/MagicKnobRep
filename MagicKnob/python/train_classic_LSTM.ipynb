{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":294},"executionInfo":{"elapsed":19515,"status":"error","timestamp":1689950266438,"user":{"displayName":"FireHead Vlogs","userId":"00714204215770433149"},"user_tz":-120},"id":"AUk8TQCYzAAe","outputId":"01a2124e-05ea-44cb-c5c3-5f792fc96650"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","[Errno 2] No such file or directory: '/content/drive/Othercomputers/Il mio MacBook Pro/MagicKnob'\n","/content\n"]},{"output_type":"error","ename":"AssertionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-1ffe21d65f9f>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# check python file folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Upload python files in {path}python\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./python'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: Upload python files in /content/drive/Othercomputers/Il mio MacBook Pro/MagicKnob/python"]}],"source":["import os\n","from datetime import datetime\n","\n","drive = True\n","\n","if drive:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    %cd /content/drive/Othercomputers/Il mio MacBook Pro/MagicKnob\n","\n","    path = \"/content/drive/Othercomputers/Il mio MacBook Pro/MagicKnob/\"\n","\n","    # check python file folder\n","    assert os.path.exists(path + \"python\"), f\"Upload python files in {path}python\"\n","    %cd ./python\n","\n","    # check data folder\n","    assert os.path.exists(path + \"data\"), f\"Upload data files in {path}data\"\n","else:\n","    path = \"../\"\n","\n","    # check python file folder\n","    assert os.path.exists(path + \"python\"), f\"Upload python files in {path}python\"\n","\n","    # check data folder\n","    assert os.path.exists(path + \"data\"), f\"Upload data files in {path}data\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wgz6WkLZzHDM"},"outputs":[],"source":["import myk_data\n","import myk_models\n","import myk_loss\n","import myk_train\n","import myk_evaluate\n","\n","import torch\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mX_cQAvgzMLR"},"outputs":[],"source":["from torch.utils.tensorboard import SummaryWriter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FXrHBuv6zQPg"},"outputs":[],"source":["# used for the writing of example outputs\n","run_name=\"audio_ht1\"\n","# dataset : need an input and output folder in this folder\n","audio_folder = f\"../data/{run_name}\"\n","#audio_folder = \"../../data/audio_ht1\"\n","assert os.path.exists(audio_folder), f\"Audio folder  not found. Looked for {audio_folder}\"\n","# used to render example output during training\n","test_file = \"../data/guitar.wav\"\n","assert os.path.exists(test_file), \"Test file not found. Looked for \" + test_file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wJPGiuLV0ylP"},"outputs":[],"source":["# initialize net specs\n","lstm_hidden_size = 32\n","learning_rate = 5e-3\n","batch_size = 50\n","max_epochs = 10000\n","\n","# create the logger for tensorboard\n","writer = SummaryWriter()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1396,"status":"ok","timestamp":1689340421912,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"izzFWaHj0zej","outputId":"aacb6450-1dcc-4b43-ac8d-f2a257b50db5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading dataset from folder  ../data/audio_ht1\n","loading input and output of ht1\n","    loading output of ht1\n","    generate_dataset:: Loaded frames from audio file 22050\n","    found input fragments of shape (120, 22050, 1)\n","    found output fragments of shape (120, 22050, 1)\n","    total input shape: (120, 22050, 1)\n","    total output shape: (120, 22050, 1)\n","\n","total input tensor shape torch.Size([120, 22050, 1])\n","total output tensor shape torch.Size([120, 22050, 1])\n","Splitting dataset\n"]}],"source":["print(\"Loading dataset from folder \", audio_folder)\n","dataset = myk_data.generate_dataset(audio_folder + \"/input/\", audio_folder + \"/output/\", frag_len_seconds=0.5)\n","\n","print(\"Splitting dataset\")\n","train_ds, val_ds, test_ds = myk_data.get_train_valid_test_datasets(dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1689340428916,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"p5wnmGGzU1KX","outputId":"bcf63d71-d831-4424-f412-05983c2fa4c9"},"outputs":[{"data":{"text/plain":["(tensor([[-3.0518e-05],\n","         [-3.0518e-05],\n","         [ 0.0000e+00],\n","         ...,\n","         [ 0.0000e+00],\n","         [ 0.0000e+00],\n","         [ 0.0000e+00]], device='cpu'),\n"," tensor([[ 0.0000e+00],\n","         [-6.6757e-06],\n","         [ 1.3351e-05],\n","         ...,\n","         [ 9.8574e-04],\n","         [ 2.4644e-03],\n","         [ 1.9716e-03]], device='cpu'))"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["dataset[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":239,"status":"ok","timestamp":1689340433928,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"KpY8DxYl1Cb0","outputId":"0af9d12b-11c3-4c5c-e46e-4a174229d9c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda device available\n"]}],"source":["# test GPU, must be done after splitting\n","device = myk_train.get_device()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D3WztACX2UOd"},"outputs":[],"source":["# create data loaders\n","train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=device))\n","val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=device))\n","test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=device))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":254,"status":"ok","timestamp":1689340451687,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"8JZL-Qqa2ORT","outputId":"dc51e24e-1fc9-4c06-fe6f-b02727f13b64"},"outputs":[{"name":"stdout","output_type":"stream","text":["SimpleLSTM(\n","  (lstm): LSTM(1, 32, batch_first=True)\n","  (dense): Linear(in_features=32, out_features=1, bias=True)\n",")\n"]}],"source":["model = myk_models.SimpleLSTM(hidden_size=lstm_hidden_size).to(device)\n","print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4CuDKuR92cxv"},"outputs":[],"source":["# crate optimizer and loss function\n","optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', factor=0.5, patience=100, verbose=True) # non lo usava\n","\n","loss_functions = myk_loss.LossWrapper()\n","\n","# https://github.com/Alec-Wright/Automated-GuitarAmpModelling/blob/main/dist_model_recnet.py\n","# https://github.com/Alec-Wright/CoreAudioML/blob/bad9469f94a2fa63a50d70ff75f5eff2208ba03f/training.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-6cmEvsqHwL7"},"outputs":[],"source":["#%load_ext tensorboard\n","#%tensorboard --logdir logs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":273693,"status":"ok","timestamp":1689340744803,"user":{"displayName":"Niccolò","userId":"04990246452610004701"},"user_tz":-120},"id":"L2_vgpXU2tvW","outputId":"873c3a6b-49e4-4cc1-a8cb-dbe2819a6a3d"},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch 0, train_loss 0.7373915910720825, val_loss 0.7491509914398193 \n","    Record loss - saving at epoch 1\n","    epoch 1, train_loss 0.6819989085197449, val_loss 0.7435746192932129 \n","    Record loss - saving at epoch 2\n","    epoch 2, train_loss 0.674778401851654, val_loss 0.7324938774108887 \n","    Record loss - saving at epoch 3\n","    epoch 3, train_loss 0.6720863580703735, val_loss 0.7257212996482849 \n","    Record loss - saving at epoch 4\n","    epoch 4, train_loss 0.6649070978164673, val_loss 0.6966635584831238 \n","    Record loss - saving at epoch 7\n","    epoch 7, train_loss 0.6114742755889893, val_loss 0.671698272228241 \n","    Record loss - saving at epoch 16\n","    epoch 16, train_loss 0.6219739317893982, val_loss 0.6656229496002197 \n","    Record loss - saving at epoch 18\n","    epoch 18, train_loss 0.6164230108261108, val_loss 0.6464424729347229 \n","    Record loss - saving at epoch 19\n","    epoch 19, train_loss 0.5969625115394592, val_loss 0.6276372671127319 \n","    Record loss - saving at epoch 39\n","    epoch 39, train_loss 0.6155499815940857, val_loss 0.6160463094711304 \n","    Record loss - saving at epoch 40\n","    epoch 40, train_loss 0.6041243076324463, val_loss 0.5892112851142883 \n","    Record loss - saving at epoch 45\n","    epoch 45, train_loss 0.5845656394958496, val_loss 0.5890099406242371 \n","epoch 50, train_loss 0.7706980109214783, val_loss 0.7439682483673096 \n","    Record loss - saving at epoch 80\n","    epoch 80, train_loss 0.5850898623466492, val_loss 0.5270748138427734 \n","epoch 100, train_loss 0.6139999032020569, val_loss 0.5913776755332947 \n","    Record loss - saving at epoch 113\n","    epoch 113, train_loss 0.49875694513320923, val_loss 0.43119168281555176 \n","    Record loss - saving at epoch 115\n","    epoch 115, train_loss 0.4148777425289154, val_loss 0.3721678853034973 \n","    Record loss - saving at epoch 116\n","    epoch 116, train_loss 0.3620307445526123, val_loss 0.36625394225120544 \n","    Record loss - saving at epoch 118\n","    epoch 118, train_loss 0.3287840187549591, val_loss 0.3410475254058838 \n","    Record loss - saving at epoch 119\n","    epoch 119, train_loss 0.31057798862457275, val_loss 0.3366849720478058 \n","    Record loss - saving at epoch 120\n","    epoch 120, train_loss 0.29942160844802856, val_loss 0.3341657519340515 \n","    Record loss - saving at epoch 121\n","    epoch 121, train_loss 0.30352067947387695, val_loss 0.3297592103481293 \n","    Record loss - saving at epoch 122\n","    epoch 122, train_loss 0.2946630120277405, val_loss 0.327655166387558 \n","    Record loss - saving at epoch 123\n","    epoch 123, train_loss 0.2855699360370636, val_loss 0.32078465819358826 \n","    Record loss - saving at epoch 125\n","    epoch 125, train_loss 0.28424161672592163, val_loss 0.3164360821247101 \n","    Record loss - saving at epoch 127\n","    epoch 127, train_loss 0.27660948038101196, val_loss 0.31582432985305786 \n","    Record loss - saving at epoch 128\n","    epoch 128, train_loss 0.2747325003147125, val_loss 0.3143273591995239 \n","    Record loss - saving at epoch 130\n","    epoch 130, train_loss 0.2741750478744507, val_loss 0.30167749524116516 \n","    Record loss - saving at epoch 131\n","    epoch 131, train_loss 0.26800960302352905, val_loss 0.29823097586631775 \n","    Record loss - saving at epoch 135\n","    epoch 135, train_loss 0.25534430146217346, val_loss 0.28480544686317444 \n","    Record loss - saving at epoch 136\n","    epoch 136, train_loss 0.2521441578865051, val_loss 0.28218337893486023 \n","    Record loss - saving at epoch 138\n","    epoch 138, train_loss 0.24798622727394104, val_loss 0.2723158597946167 \n","    Record loss - saving at epoch 139\n","    epoch 139, train_loss 0.24114294350147247, val_loss 0.2642673850059509 \n","    Record loss - saving at epoch 140\n","    epoch 140, train_loss 0.2353348284959793, val_loss 0.2559058964252472 \n","    Record loss - saving at epoch 141\n","    epoch 141, train_loss 0.23283930122852325, val_loss 0.2553330361843109 \n","    Record loss - saving at epoch 142\n","    epoch 142, train_loss 0.22569997608661652, val_loss 0.2421640157699585 \n","    Record loss - saving at epoch 150\n","    epoch 150, train_loss 0.21213440597057343, val_loss 0.23472817242145538 \n","epoch 150, train_loss 0.21213440597057343, val_loss 0.23472817242145538 \n","    Record loss - saving at epoch 151\n","    epoch 151, train_loss 0.20683228969573975, val_loss 0.22891272604465485 \n","    Record loss - saving at epoch 152\n","    epoch 152, train_loss 0.20650044083595276, val_loss 0.22474102675914764 \n","    Record loss - saving at epoch 153\n","    epoch 153, train_loss 0.20639212429523468, val_loss 0.22048071026802063 \n","    Record loss - saving at epoch 154\n","    epoch 154, train_loss 0.20247459411621094, val_loss 0.2200612872838974 \n","    Record loss - saving at epoch 155\n","    epoch 155, train_loss 0.19528937339782715, val_loss 0.21557419002056122 \n","    Record loss - saving at epoch 156\n","    epoch 156, train_loss 0.19441479444503784, val_loss 0.21199508011341095 \n","    Record loss - saving at epoch 158\n","    epoch 158, train_loss 0.20096245408058167, val_loss 0.20820380747318268 \n","    Record loss - saving at epoch 159\n","    epoch 159, train_loss 0.1935286819934845, val_loss 0.20351868867874146 \n","    Record loss - saving at epoch 161\n","    epoch 161, train_loss 0.18320782482624054, val_loss 0.19848087430000305 \n","    Record loss - saving at epoch 162\n","    epoch 162, train_loss 0.1840052306652069, val_loss 0.19631154835224152 \n","    Record loss - saving at epoch 166\n","    epoch 166, train_loss 0.17922811210155487, val_loss 0.1919255405664444 \n","    Record loss - saving at epoch 167\n","    epoch 167, train_loss 0.17759259045124054, val_loss 0.18732814490795135 \n","    Record loss - saving at epoch 168\n","    epoch 168, train_loss 0.17502503097057343, val_loss 0.1837541162967682 \n","    Record loss - saving at epoch 169\n","    epoch 169, train_loss 0.16930177807807922, val_loss 0.17430613934993744 \n","    Record loss - saving at epoch 173\n","    epoch 173, train_loss 0.16172663867473602, val_loss 0.16765201091766357 \n","    Record loss - saving at epoch 174\n","    epoch 174, train_loss 0.1613268256187439, val_loss 0.16658660769462585 \n","    Record loss - saving at epoch 176\n","    epoch 176, train_loss 0.15435373783111572, val_loss 0.15922385454177856 \n","    Record loss - saving at epoch 178\n","    epoch 178, train_loss 0.15179410576820374, val_loss 0.1573534607887268 \n","    Record loss - saving at epoch 181\n","    epoch 181, train_loss 0.15095481276512146, val_loss 0.15573683381080627 \n","    Record loss - saving at epoch 184\n","    epoch 184, train_loss 0.14957565069198608, val_loss 0.1546204686164856 \n","    Record loss - saving at epoch 185\n","    epoch 185, train_loss 0.14860041439533234, val_loss 0.15371516346931458 \n","    Record loss - saving at epoch 187\n","    epoch 187, train_loss 0.14688804745674133, val_loss 0.15360146760940552 \n","    Record loss - saving at epoch 188\n","    epoch 188, train_loss 0.14483866095542908, val_loss 0.15147429704666138 \n","    Record loss - saving at epoch 189\n","    epoch 189, train_loss 0.141087144613266, val_loss 0.14782150089740753 \n","    Record loss - saving at epoch 190\n","    epoch 190, train_loss 0.14205430448055267, val_loss 0.145918607711792 \n","    Record loss - saving at epoch 191\n","    epoch 191, train_loss 0.14066064357757568, val_loss 0.1430090218782425 \n","    Record loss - saving at epoch 194\n","    epoch 194, train_loss 0.13812220096588135, val_loss 0.13938526809215546 \n","epoch 200, train_loss 0.13340941071510315, val_loss 0.14012357592582703 \n","    Record loss - saving at epoch 203\n","    epoch 203, train_loss 0.13410279154777527, val_loss 0.1364717036485672 \n","    Record loss - saving at epoch 204\n","    epoch 204, train_loss 0.12992678582668304, val_loss 0.13609425723552704 \n","    Record loss - saving at epoch 206\n","    epoch 206, train_loss 0.13070440292358398, val_loss 0.13535934686660767 \n","    Record loss - saving at epoch 207\n","    epoch 207, train_loss 0.13022252917289734, val_loss 0.13285917043685913 \n","    Record loss - saving at epoch 208\n","    epoch 208, train_loss 0.12811388075351715, val_loss 0.12944476306438446 \n","    Record loss - saving at epoch 209\n","    epoch 209, train_loss 0.12891514599323273, val_loss 0.12851981818675995 \n","    Record loss - saving at epoch 212\n","    epoch 212, train_loss 0.12661762535572052, val_loss 0.1269342005252838 \n","    Record loss - saving at epoch 215\n","    epoch 215, train_loss 0.12510700523853302, val_loss 0.12462775409221649 \n","    Record loss - saving at epoch 220\n","    epoch 220, train_loss 0.12310949712991714, val_loss 0.12189122289419174 \n","    Record loss - saving at epoch 223\n","    epoch 223, train_loss 0.12484830617904663, val_loss 0.1216004490852356 \n","    Record loss - saving at epoch 224\n","    epoch 224, train_loss 0.12076370418071747, val_loss 0.11875346302986145 \n","    Record loss - saving at epoch 226\n","    epoch 226, train_loss 0.11966140568256378, val_loss 0.11691301316022873 \n","    Record loss - saving at epoch 231\n","    epoch 231, train_loss 0.11962791532278061, val_loss 0.11128298193216324 \n","    Record loss - saving at epoch 233\n","    epoch 233, train_loss 0.11697618663311005, val_loss 0.10805854201316833 \n","    Record loss - saving at epoch 234\n","    epoch 234, train_loss 0.11678018420934677, val_loss 0.10667049139738083 \n","    Record loss - saving at epoch 240\n","    epoch 240, train_loss 0.11316891014575958, val_loss 0.10527102649211884 \n","    Record loss - saving at epoch 241\n","    epoch 241, train_loss 0.1132679134607315, val_loss 0.10301347821950912 \n","    Record loss - saving at epoch 243\n","    epoch 243, train_loss 0.11201751232147217, val_loss 0.10103339701890945 \n","    Record loss - saving at epoch 248\n","    epoch 248, train_loss 0.11193130165338516, val_loss 0.10100795328617096 \n","epoch 250, train_loss 0.11003462225198746, val_loss 0.10157383233308792 \n","    Record loss - saving at epoch 254\n","    epoch 254, train_loss 0.10898320376873016, val_loss 0.09711506217718124 \n","    Record loss - saving at epoch 256\n","    epoch 256, train_loss 0.10862239450216293, val_loss 0.09150711447000504 \n","    Record loss - saving at epoch 263\n","    epoch 263, train_loss 0.10177651792764664, val_loss 0.08857781440019608 \n","    Record loss - saving at epoch 278\n","    epoch 278, train_loss 0.10123254358768463, val_loss 0.08849534392356873 \n","    Record loss - saving at epoch 289\n","    epoch 289, train_loss 0.10100104659795761, val_loss 0.08522658050060272 \n","    Record loss - saving at epoch 293\n","    epoch 293, train_loss 0.09867013990879059, val_loss 0.08460669964551926 \n","    Record loss - saving at epoch 295\n","    epoch 295, train_loss 0.10010440647602081, val_loss 0.0808030366897583 \n","epoch 300, train_loss 0.0962991714477539, val_loss 0.08336161822080612 \n","    Record loss - saving at epoch 303\n","    epoch 303, train_loss 0.09510710090398788, val_loss 0.07909934222698212 \n","    Record loss - saving at epoch 304\n","    epoch 304, train_loss 0.0957208052277565, val_loss 0.07899037003517151 \n","    Record loss - saving at epoch 309\n","    epoch 309, train_loss 0.09330747276544571, val_loss 0.07830291241407394 \n","    Record loss - saving at epoch 315\n","    epoch 315, train_loss 0.09278207272291183, val_loss 0.07432977110147476 \n","    Record loss - saving at epoch 321\n","    epoch 321, train_loss 0.09122294932603836, val_loss 0.07383616268634796 \n","    Record loss - saving at epoch 330\n","    epoch 330, train_loss 0.09155736863613129, val_loss 0.07336762547492981 \n","    Record loss - saving at epoch 331\n","    epoch 331, train_loss 0.08978323638439178, val_loss 0.07189493626356125 \n","    Record loss - saving at epoch 338\n","    epoch 338, train_loss 0.09063231945037842, val_loss 0.0703081339597702 \n","    Record loss - saving at epoch 347\n","    epoch 347, train_loss 0.08623969554901123, val_loss 0.06716155260801315 \n","epoch 350, train_loss 0.08711467683315277, val_loss 0.06913597881793976 \n","    Record loss - saving at epoch 359\n","    epoch 359, train_loss 0.08547873795032501, val_loss 0.06664002686738968 \n","    Record loss - saving at epoch 367\n","    epoch 367, train_loss 0.08566109836101532, val_loss 0.06474809348583221 \n","    Record loss - saving at epoch 374\n","    epoch 374, train_loss 0.08428274840116501, val_loss 0.06326013803482056 \n","    Record loss - saving at epoch 396\n","    epoch 396, train_loss 0.08236713707447052, val_loss 0.06273525208234787 \n","epoch 400, train_loss 0.08165767788887024, val_loss 0.06506088376045227 \n","    Record loss - saving at epoch 403\n","    epoch 403, train_loss 0.082819864153862, val_loss 0.06183239817619324 \n","    Record loss - saving at epoch 404\n","    epoch 404, train_loss 0.0829438716173172, val_loss 0.06133006513118744 \n","    Record loss - saving at epoch 408\n","    epoch 408, train_loss 0.08207114040851593, val_loss 0.06122758239507675 \n","    Record loss - saving at epoch 414\n","    epoch 414, train_loss 0.08174149692058563, val_loss 0.05987647920846939 \n","    Record loss - saving at epoch 416\n","    epoch 416, train_loss 0.08216233551502228, val_loss 0.05846123397350311 \n","    Record loss - saving at epoch 420\n","    epoch 420, train_loss 0.0813976377248764, val_loss 0.05830276384949684 \n","epoch 450, train_loss 0.08174589276313782, val_loss 0.06227252259850502 \n","    Record loss - saving at epoch 473\n","    epoch 473, train_loss 0.07887580245733261, val_loss 0.05676953122019768 \n","epoch 500, train_loss 0.07809925824403763, val_loss 0.0594889298081398 \n","    Record loss - saving at epoch 501\n","    epoch 501, train_loss 0.07698795199394226, val_loss 0.05514654144644737 \n","    Record loss - saving at epoch 502\n","    epoch 502, train_loss 0.0749034658074379, val_loss 0.054244816303253174 \n","    Record loss - saving at epoch 520\n","    epoch 520, train_loss 0.07435078173875809, val_loss 0.05424048379063606 \n","    Record loss - saving at epoch 522\n","    epoch 522, train_loss 0.07454339414834976, val_loss 0.054172009229660034 \n","    Record loss - saving at epoch 523\n","    epoch 523, train_loss 0.07382943481206894, val_loss 0.05229061096906662 \n","    Record loss - saving at epoch 527\n","    epoch 527, train_loss 0.0742541030049324, val_loss 0.051077742129564285 \n","epoch 550, train_loss 0.07282550632953644, val_loss 0.05241500586271286 \n","    Record loss - saving at epoch 554\n","    epoch 554, train_loss 0.07324614375829697, val_loss 0.05032441392540932 \n","    Record loss - saving at epoch 557\n","    epoch 557, train_loss 0.07384277135133743, val_loss 0.04966679587960243 \n","    Record loss - saving at epoch 558\n","    epoch 558, train_loss 0.07155975699424744, val_loss 0.04956446588039398 \n","    Record loss - saving at epoch 569\n","    epoch 569, train_loss 0.07291092723608017, val_loss 0.04905765503644943 \n","    Record loss - saving at epoch 572\n","    epoch 572, train_loss 0.0710475817322731, val_loss 0.04714765399694443 \n","epoch 600, train_loss 0.0711694210767746, val_loss 0.052851445972919464 \n","    Record loss - saving at epoch 601\n","    epoch 601, train_loss 0.07089284062385559, val_loss 0.04690728709101677 \n","epoch 650, train_loss 0.160587340593338, val_loss 0.1742919683456421 \n","epoch 700, train_loss 0.10835577547550201, val_loss 0.11345133185386658 \n","Epoch 00703: reducing learning rate of group 0 to 2.5000e-03.\n","epoch 750, train_loss 0.09203524887561798, val_loss 0.08485378324985504 \n","epoch 800, train_loss 0.08676964789628983, val_loss 0.07006057351827621 \n","Epoch 00804: reducing learning rate of group 0 to 1.2500e-03.\n","epoch 850, train_loss 0.080123171210289, val_loss 0.06464608013629913 \n","epoch 900, train_loss 0.07832092046737671, val_loss 0.06370934844017029 \n","Epoch 00905: reducing learning rate of group 0 to 6.2500e-04.\n","epoch 950, train_loss 0.07477737218141556, val_loss 0.05934985727071762 \n","epoch 1000, train_loss 0.0737188383936882, val_loss 0.05767931044101715 \n","Epoch 01006: reducing learning rate of group 0 to 3.1250e-04.\n","epoch 1050, train_loss 0.07204245030879974, val_loss 0.05607140064239502 \n","epoch 1100, train_loss 0.07118873298168182, val_loss 0.05550670251250267 \n","max patience reached, stopping training\n"]}],"source":["# training loop\n","lowest_val_loss = 0\n","best_loss = False\n","\n","max_patience = 500\n","curr_patience = max_patience\n","\n","# datetime object containing current date and time\n","best_epoch = 0\n","now = datetime.now()\n","dt_string = now.strftime(f\"models/{model.model_type}_%d-%m-%Y_%H-%M-%S\")\n","\n","os.mkdir(os.path.join(\".\", dt_string))\n","\n","with open(f'{dt_string}/model_structure.txt', \"w\") as model_structure:\n","    model_structure.write(str(model))\n","\n","for epoch in range(max_epochs):\n","    #ep_loss = myk_train.train_epoch_interval(model, train_dl, loss_functions, optimiser, device=device)\n","    ep_loss = myk_train.train_epoch_interval(model, train_dl, loss_functions, optimiser, device=device)\n","\n","    #ep_loss = myk_train.train_epoch(model, train_dl, loss_functions, optimiser, device=device)\n","    val_loss = myk_train.compute_batch_loss(model, val_dl, loss_functions, device=device)\n","\n","    scheduler.step(val_loss)\n","\n","    writer.add_scalar(\"Loss/val\", val_loss, epoch)\n","    writer.add_scalar(\"Loss/train\", ep_loss, epoch)\n","\n","    # check if we have beaten our best loss to date\n","    if lowest_val_loss == 0: # first run\n","        lowest_val_loss = val_loss\n","    elif val_loss < lowest_val_loss:# new record\n","        lowest_val_loss = val_loss\n","        best_loss = True\n","    else: # no improvement\n","        best_loss = False\n","        curr_patience -= 1\n","\n","    if best_loss: # save best model so far\n","        best_epoch = epoch\n","        print(f\"    Record loss - saving at epoch {epoch}\")\n","        # save for RTNeural\n","        model.save_for_rtneural(f\"{dt_string}/model.json\")\n","        # save for pythorch\n","        torch.save(model.state_dict(), f\"{dt_string}/model.ph\")\n","        print(f\"    epoch {epoch}, train_loss {ep_loss}, val_loss {val_loss} \")\n","        curr_patience = max_patience\n","    if epoch % 50 == 0: # save an example processed audio file\n","        myk_evaluate.run_file_through_model(model, test_file, audio_folder + \"/\" + run_name + str(epoch)+\".wav\")\n","        print(f\"epoch {epoch}, train_loss {ep_loss}, val_loss {val_loss} \")\n","    if curr_patience == 0:\n","        print(\"max patience reached, stopping training\")\n","        # load best parameters in the model\n","        model.load_state_dict(torch.load(f\"{dt_string}/model.ph\"))\n","        model.eval() # set inference state in the possible layers that need it\n","        myk_evaluate.run_file_through_model(model, test_file, audio_folder + \"/\" + run_name + str(best_epoch)+\"_BEST.wav\")\n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"edQeVKnKqIlY"},"outputs":[],"source":["to_append = [f'\\n{run_name}/*.wav', f'\\n!{run_name}/*_BEST*.wav']\n","\n","with open(f'{audio_folder}/../.gitignore', \"r+\") as gitignore:\n","    read = gitignore.read()\n","    if to_append[0][2:len(to_append[0])] not in read:\n","        gitignore.writelines(to_append)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3wYTWml9kSD1"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1R90W11c0RiDl3RfZT-sX8pRNVVVbmSYh","timestamp":1685016693193}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":0}