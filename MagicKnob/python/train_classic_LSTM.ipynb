{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3719,"status":"ok","timestamp":1689951509340,"user":{"displayName":"FireHead Vlogs","userId":"00714204215770433149"},"user_tz":-120},"id":"AUk8TQCYzAAe","outputId":"9b6b1db6-ee3d-4032-dc67-db21b103b6c8"},"outputs":[{"name":"stdout","output_type":"stream","text":["/Users/macdonald/Desktop/MagicKnobRep/MagicKnob/python\n"]}],"source":["import os\n","from datetime import datetime\n","\n","working_path = os.getcwd()\n","print(working_path)\n","\n","if working_path == '/content':\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    %cd /content/drive/Othercomputers/Il mio MacBook Pro/MagicKnob\n","\n","    path = \"/content/drive/Othercomputers/Il mio MacBook Pro/MagicKnob/\"\n","\n","    # check python file folder\n","    assert os.path.exists(path + \"python\"), f\"Upload python files in {path}python\"\n","    %cd ./python\n","\n","    # check data folder\n","    assert os.path.exists(path + \"data\"), f\"Upload data files in {path}data\"\n","else:\n","    path = \"../\"\n","\n","    # check python file folder\n","    assert os.path.exists(path + \"python\"), f\"Upload python files in {path}python\"\n","\n","    # check data folder\n","    assert os.path.exists(path + \"data\"), f\"Upload data files in {path}data\""]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Wgz6WkLZzHDM"},"outputs":[],"source":["import myk_data\n","import myk_models\n","import myk_loss\n","import myk_train\n","import myk_evaluate\n","\n","import torch\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"mX_cQAvgzMLR"},"outputs":[],"source":["from torch.utils.tensorboard import SummaryWriter"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"FXrHBuv6zQPg"},"outputs":[],"source":["# used for the writing of example outputs\n","run_name=\"audio_lstm_lpf_par\"\n","# dataset : need an input and output folder in this folder\n","audio_folder = f\"../data/{run_name}\"\n","#audio_folder = \"../../data/audio_ht1\"\n","assert os.path.exists(audio_folder), f\"Audio folder  not found. Looked for {audio_folder}\"\n","# used to render example output during training\n","test_file = \"../data/guitar.wav\"\n","assert os.path.exists(test_file), \"Test file not found. Looked for \" + test_file"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"wJPGiuLV0ylP"},"outputs":[],"source":["# initialize net specs\n","lstm_hidden_size = 16\n","learning_rate = 5e-3\n","batch_size = 50\n","max_epochs = 10000\n","\n","# create the logger for tensorboard\n","writer = SummaryWriter()"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3907,"status":"ok","timestamp":1689951578489,"user":{"displayName":"FireHead Vlogs","userId":"00714204215770433149"},"user_tz":-120},"id":"izzFWaHj0zej","outputId":"eec7e082-9016-4ad3-c834-1690861a85e0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading dataset from folder  ../data/audio_lstm_lpf_par\n","loading input and output of filter\n"]},{"ename":"AssertionError","evalue":"incorrect target naming convention, please name the outputs: name-target.wav","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoading dataset from folder \u001b[39m\u001b[39m\"\u001b[39m, audio_folder)\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[39m=\u001b[39m myk_data\u001b[39m.\u001b[39;49mgenerate_dataset(audio_folder \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m/input/\u001b[39;49m\u001b[39m\"\u001b[39;49m, audio_folder \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m/output/\u001b[39;49m\u001b[39m\"\u001b[39;49m, frag_len_seconds\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSplitting dataset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m train_ds, val_ds, test_ds \u001b[39m=\u001b[39m myk_data\u001b[39m.\u001b[39mget_train_valid_test_datasets(dataset)\n","File \u001b[0;32m~/Desktop/MagicKnobRep/MagicKnob/python/myk_data.py:116\u001b[0m, in \u001b[0;36mgenerate_dataset\u001b[0;34m(input_audio_folder, output_audio_folder, frag_len_seconds, samplerate)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading input and output of \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[39m# dictionarization\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39m# name: filename\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m output_dict \u001b[39m=\u001b[39m {\n\u001b[1;32m    117\u001b[0m     output_file_name\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m0\u001b[39;49m]: output_file_name \n\u001b[1;32m    118\u001b[0m         \u001b[39mfor\u001b[39;49;00m output_file_name \u001b[39min\u001b[39;49;00m output_files \n\u001b[1;32m    119\u001b[0m         \u001b[39mif\u001b[39;49;00m check_output_naming(output_file_name, name)\n\u001b[1;32m    120\u001b[0m }\n\u001b[1;32m    122\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(output_dict) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(input_dict), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfound a number of output != 1: \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m    loading output of \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m~/Desktop/MagicKnobRep/MagicKnob/python/myk_data.py:119\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading input and output of \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[39m# dictionarization\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39m# name: filename\u001b[39;00m\n\u001b[1;32m    116\u001b[0m output_dict \u001b[39m=\u001b[39m {\n\u001b[1;32m    117\u001b[0m     output_file_name\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m]: output_file_name \n\u001b[1;32m    118\u001b[0m         \u001b[39mfor\u001b[39;00m output_file_name \u001b[39min\u001b[39;00m output_files \n\u001b[0;32m--> 119\u001b[0m         \u001b[39mif\u001b[39;00m check_output_naming(output_file_name, name)\n\u001b[1;32m    120\u001b[0m }\n\u001b[1;32m    122\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(output_dict) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(input_dict), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfound a number of output != 1: \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m    loading output of \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m~/Desktop/MagicKnobRep/MagicKnob/python/myk_data.py:182\u001b[0m, in \u001b[0;36mcheck_output_naming\u001b[0;34m(output_file_path, name)\u001b[0m\n\u001b[1;32m    179\u001b[0m split \u001b[39m=\u001b[39m output_file_name\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    181\u001b[0m \u001b[39m# check structure and name\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(split) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mincorrect target naming convention, please name the outputs: name-target.wav\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    184\u001b[0m \u001b[39mif\u001b[39;00m split[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m name:\n\u001b[1;32m    185\u001b[0m     \u001b[39m# check target keyward\u001b[39;00m\n\u001b[1;32m    186\u001b[0m     \u001b[39massert\u001b[39;00m output_file_name \u001b[39m==\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m-target.wav\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mincorrect target naming convention, please name the outputs: name-target.wav\u001b[39m\u001b[39m\"\u001b[39m\n","\u001b[0;31mAssertionError\u001b[0m: incorrect target naming convention, please name the outputs: name-target.wav"]}],"source":["print(\"Loading dataset from folder \", audio_folder)\n","dataset = myk_data.generate_dataset(audio_folder + \"/input/\", audio_folder + \"/output/\", frag_len_seconds=0.5)\n","\n","print(\"Splitting dataset\")\n","train_ds, val_ds, test_ds = myk_data.get_train_valid_test_datasets(dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":429,"status":"ok","timestamp":1689951581613,"user":{"displayName":"FireHead Vlogs","userId":"00714204215770433149"},"user_tz":-120},"id":"p5wnmGGzU1KX","outputId":"5b2ed790-25b3-4180-80df-96287cb5c718"},"outputs":[{"data":{"text/plain":["(tensor([[-3.0518e-05],\n","         [-3.0518e-05],\n","         [ 0.0000e+00],\n","         ...,\n","         [ 0.0000e+00],\n","         [ 0.0000e+00],\n","         [ 0.0000e+00]]),\n"," tensor([[ 0.0000e+00],\n","         [-6.6757e-06],\n","         [ 1.3351e-05],\n","         ...,\n","         [ 9.8574e-04],\n","         [ 2.4644e-03],\n","         [ 1.9716e-03]]))"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["dataset[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1022,"status":"ok","timestamp":1689951589979,"user":{"displayName":"FireHead Vlogs","userId":"00714204215770433149"},"user_tz":-120},"id":"KpY8DxYl1Cb0","outputId":"ac5f88bf-54b9-4b4e-a58b-36eebc6a9424"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda device available\n"]}],"source":["# test GPU, must be done after splitting\n","device = myk_train.get_device()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D3WztACX2UOd"},"outputs":[],"source":["# create data loaders\n","train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=device))\n","val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=device))\n","test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=device))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":476,"status":"ok","timestamp":1689951732618,"user":{"displayName":"FireHead Vlogs","userId":"00714204215770433149"},"user_tz":-120},"id":"8JZL-Qqa2ORT","outputId":"50d3767f-f50a-4881-eed3-b9d8df0d4dc2"},"outputs":[{"name":"stdout","output_type":"stream","text":["SimpleLSTM(\n","  (lstm): LSTM(1, 16, batch_first=True)\n","  (dense): Linear(in_features=16, out_features=1, bias=True)\n",")\n"]},{"data":{"text/plain":["(None, 1233)"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["model = myk_models.SimpleLSTM(hidden_size=lstm_hidden_size).to(device)\n","total_params = sum(p.numel() for p in model.parameters())\n","print(model), total_params"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4CuDKuR92cxv"},"outputs":[],"source":["# crate optimizer and loss function\n","optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', factor=0.5, patience=100, verbose=True) # non lo usava\n","\n","loss_functions = myk_loss.LossWrapper()\n","\n","# https://github.com/Alec-Wright/Automated-GuitarAmpModelling/blob/main/dist_model_recnet.py\n","# https://github.com/Alec-Wright/CoreAudioML/blob/bad9469f94a2fa63a50d70ff75f5eff2208ba03f/training.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-6cmEvsqHwL7"},"outputs":[],"source":["#%load_ext tensorboard\n","#%tensorboard --logdir logs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":623535,"status":"ok","timestamp":1689952447350,"user":{"displayName":"FireHead Vlogs","userId":"00714204215770433149"},"user_tz":-120},"id":"L2_vgpXU2tvW","outputId":"9fe53979-b95c-4a90-8a1a-a828687e99c3"},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch 0, train_loss 0.7989652156829834, val_loss 0.7657268643379211 \n","    Record loss - saving at epoch 1\n","    epoch 1, train_loss 0.7510331869125366, val_loss 0.7562835216522217 \n","    Record loss - saving at epoch 2\n","    epoch 2, train_loss 0.7437922358512878, val_loss 0.743682861328125 \n","    Record loss - saving at epoch 3\n","    epoch 3, train_loss 0.7346034049987793, val_loss 0.7177652716636658 \n","    Record loss - saving at epoch 4\n","    epoch 4, train_loss 0.709241509437561, val_loss 0.7099441885948181 \n","    Record loss - saving at epoch 5\n","    epoch 5, train_loss 0.6994122266769409, val_loss 0.694219172000885 \n","    Record loss - saving at epoch 7\n","    epoch 7, train_loss 0.6797970533370972, val_loss 0.6889298558235168 \n","    Record loss - saving at epoch 12\n","    epoch 12, train_loss 0.6326566934585571, val_loss 0.6766320466995239 \n","    Record loss - saving at epoch 13\n","    epoch 13, train_loss 0.614849865436554, val_loss 0.6680309772491455 \n","    Record loss - saving at epoch 15\n","    epoch 15, train_loss 0.608484148979187, val_loss 0.6589358448982239 \n","    Record loss - saving at epoch 16\n","    epoch 16, train_loss 0.5906442999839783, val_loss 0.6462235450744629 \n","    Record loss - saving at epoch 18\n","    epoch 18, train_loss 0.5918927192687988, val_loss 0.6119226813316345 \n","    Record loss - saving at epoch 19\n","    epoch 19, train_loss 0.5601479411125183, val_loss 0.5552949905395508 \n","epoch 50, train_loss 0.6869814395904541, val_loss 0.68074631690979 \n","    Record loss - saving at epoch 71\n","    epoch 71, train_loss 0.5504319071769714, val_loss 0.5530121922492981 \n","    Record loss - saving at epoch 72\n","    epoch 72, train_loss 0.5278812050819397, val_loss 0.5363502502441406 \n","    Record loss - saving at epoch 73\n","    epoch 73, train_loss 0.5411996245384216, val_loss 0.5349507927894592 \n","    Record loss - saving at epoch 74\n","    epoch 74, train_loss 0.5158407688140869, val_loss 0.49042555689811707 \n","    Record loss - saving at epoch 75\n","    epoch 75, train_loss 0.5122222900390625, val_loss 0.4897197186946869 \n","    Record loss - saving at epoch 76\n","    epoch 76, train_loss 0.5039981007575989, val_loss 0.452251672744751 \n","    Record loss - saving at epoch 77\n","    epoch 77, train_loss 0.4995179772377014, val_loss 0.43952569365501404 \n","    Record loss - saving at epoch 78\n","    epoch 78, train_loss 0.4789351522922516, val_loss 0.4256303310394287 \n","    Record loss - saving at epoch 79\n","    epoch 79, train_loss 0.47343260049819946, val_loss 0.4074324369430542 \n","    Record loss - saving at epoch 80\n","    epoch 80, train_loss 0.4697934687137604, val_loss 0.40666821599006653 \n","    Record loss - saving at epoch 81\n","    epoch 81, train_loss 0.4622734487056732, val_loss 0.39903807640075684 \n","    Record loss - saving at epoch 83\n","    epoch 83, train_loss 0.45726123452186584, val_loss 0.39376065135002136 \n","    Record loss - saving at epoch 84\n","    epoch 84, train_loss 0.4472329616546631, val_loss 0.388109028339386 \n","    Record loss - saving at epoch 87\n","    epoch 87, train_loss 0.4535368084907532, val_loss 0.3631473183631897 \n","    Record loss - saving at epoch 93\n","    epoch 93, train_loss 0.37784767150878906, val_loss 0.3535171449184418 \n","    Record loss - saving at epoch 94\n","    epoch 94, train_loss 0.3726879954338074, val_loss 0.3355428874492645 \n","    Record loss - saving at epoch 95\n","    epoch 95, train_loss 0.3748616576194763, val_loss 0.3306674063205719 \n","    Record loss - saving at epoch 97\n","    epoch 97, train_loss 0.3717849850654602, val_loss 0.3231140375137329 \n","    Record loss - saving at epoch 99\n","    epoch 99, train_loss 0.36331453919410706, val_loss 0.3226824402809143 \n","    Record loss - saving at epoch 100\n","    epoch 100, train_loss 0.3611411154270172, val_loss 0.3175440728664398 \n","epoch 100, train_loss 0.3611411154270172, val_loss 0.3175440728664398 \n","    Record loss - saving at epoch 101\n","    epoch 101, train_loss 0.35099223256111145, val_loss 0.31582877039909363 \n","    Record loss - saving at epoch 102\n","    epoch 102, train_loss 0.3524332046508789, val_loss 0.313279926776886 \n","    Record loss - saving at epoch 103\n","    epoch 103, train_loss 0.3505825102329254, val_loss 0.3101310133934021 \n","    Record loss - saving at epoch 105\n","    epoch 105, train_loss 0.3480752408504486, val_loss 0.3056294023990631 \n","    Record loss - saving at epoch 107\n","    epoch 107, train_loss 0.34237295389175415, val_loss 0.29982635378837585 \n","    Record loss - saving at epoch 112\n","    epoch 112, train_loss 0.3391505777835846, val_loss 0.29398006200790405 \n","    Record loss - saving at epoch 115\n","    epoch 115, train_loss 0.3272845447063446, val_loss 0.28640201687812805 \n","    Record loss - saving at epoch 119\n","    epoch 119, train_loss 0.31864413619041443, val_loss 0.2847039997577667 \n","    Record loss - saving at epoch 120\n","    epoch 120, train_loss 0.31363290548324585, val_loss 0.27754610776901245 \n","    Record loss - saving at epoch 122\n","    epoch 122, train_loss 0.3161231279373169, val_loss 0.27453309297561646 \n","    Record loss - saving at epoch 124\n","    epoch 124, train_loss 0.3100931942462921, val_loss 0.27373790740966797 \n","    Record loss - saving at epoch 126\n","    epoch 126, train_loss 0.30774781107902527, val_loss 0.269123375415802 \n","    Record loss - saving at epoch 128\n","    epoch 128, train_loss 0.30081871151924133, val_loss 0.26819536089897156 \n","    Record loss - saving at epoch 129\n","    epoch 129, train_loss 0.2982647716999054, val_loss 0.2657235562801361 \n","    Record loss - saving at epoch 131\n","    epoch 131, train_loss 0.291472852230072, val_loss 0.26270008087158203 \n","    Record loss - saving at epoch 133\n","    epoch 133, train_loss 0.2917962670326233, val_loss 0.26126664876937866 \n","    Record loss - saving at epoch 134\n","    epoch 134, train_loss 0.29038387537002563, val_loss 0.2523512840270996 \n","    Record loss - saving at epoch 136\n","    epoch 136, train_loss 0.2822245955467224, val_loss 0.248659148812294 \n","    Record loss - saving at epoch 142\n","    epoch 142, train_loss 0.27292484045028687, val_loss 0.24649883806705475 \n","    Record loss - saving at epoch 143\n","    epoch 143, train_loss 0.27020740509033203, val_loss 0.2398599237203598 \n","    Record loss - saving at epoch 146\n","    epoch 146, train_loss 0.2707698345184326, val_loss 0.23635636270046234 \n","epoch 150, train_loss 0.26213082671165466, val_loss 0.2541190981864929 \n","    Record loss - saving at epoch 151\n","    epoch 151, train_loss 0.2647347152233124, val_loss 0.23622262477874756 \n","    Record loss - saving at epoch 154\n","    epoch 154, train_loss 0.2626020312309265, val_loss 0.2336289882659912 \n","    Record loss - saving at epoch 158\n","    epoch 158, train_loss 0.26426294445991516, val_loss 0.23334093391895294 \n","    Record loss - saving at epoch 161\n","    epoch 161, train_loss 0.2563038766384125, val_loss 0.22568568587303162 \n","    Record loss - saving at epoch 169\n","    epoch 169, train_loss 0.2501384913921356, val_loss 0.22472049295902252 \n","    Record loss - saving at epoch 170\n","    epoch 170, train_loss 0.2463943511247635, val_loss 0.22460047900676727 \n","    Record loss - saving at epoch 171\n","    epoch 171, train_loss 0.24366526305675507, val_loss 0.21996775269508362 \n","    Record loss - saving at epoch 173\n","    epoch 173, train_loss 0.24810972809791565, val_loss 0.21662737429141998 \n","    Record loss - saving at epoch 176\n","    epoch 176, train_loss 0.24378618597984314, val_loss 0.21461695432662964 \n","    Record loss - saving at epoch 182\n","    epoch 182, train_loss 0.23163802921772003, val_loss 0.21283897757530212 \n","    Record loss - saving at epoch 183\n","    epoch 183, train_loss 0.23714454472064972, val_loss 0.21194694936275482 \n","    Record loss - saving at epoch 186\n","    epoch 186, train_loss 0.23372483253479004, val_loss 0.2092861533164978 \n","    Record loss - saving at epoch 189\n","    epoch 189, train_loss 0.2458711564540863, val_loss 0.20569521188735962 \n","    Record loss - saving at epoch 192\n","    epoch 192, train_loss 0.22600321471691132, val_loss 0.2039446085691452 \n","    Record loss - saving at epoch 197\n","    epoch 197, train_loss 0.2246684730052948, val_loss 0.2020939290523529 \n","    Record loss - saving at epoch 199\n","    epoch 199, train_loss 0.22555029392242432, val_loss 0.20134802162647247 \n","    Record loss - saving at epoch 200\n","    epoch 200, train_loss 0.2273377925157547, val_loss 0.20073452591896057 \n","epoch 200, train_loss 0.2273377925157547, val_loss 0.20073452591896057 \n","    Record loss - saving at epoch 202\n","    epoch 202, train_loss 0.2221938818693161, val_loss 0.19429068267345428 \n","    Record loss - saving at epoch 203\n","    epoch 203, train_loss 0.21822945773601532, val_loss 0.19090230762958527 \n","    Record loss - saving at epoch 205\n","    epoch 205, train_loss 0.21930734813213348, val_loss 0.18840640783309937 \n","    Record loss - saving at epoch 211\n","    epoch 211, train_loss 0.21471205353736877, val_loss 0.18393592536449432 \n","    Record loss - saving at epoch 214\n","    epoch 214, train_loss 0.21052250266075134, val_loss 0.18136845529079437 \n","    Record loss - saving at epoch 223\n","    epoch 223, train_loss 0.20441703498363495, val_loss 0.17613521218299866 \n","    Record loss - saving at epoch 225\n","    epoch 225, train_loss 0.20469243824481964, val_loss 0.17544803023338318 \n","    Record loss - saving at epoch 226\n","    epoch 226, train_loss 0.2040380984544754, val_loss 0.1740138679742813 \n","    Record loss - saving at epoch 227\n","    epoch 227, train_loss 0.20107464492321014, val_loss 0.1662338227033615 \n","epoch 250, train_loss 0.1957819014787674, val_loss 0.17235325276851654 \n","    Record loss - saving at epoch 257\n","    epoch 257, train_loss 0.19560854136943817, val_loss 0.16282159090042114 \n","    Record loss - saving at epoch 265\n","    epoch 265, train_loss 0.19105656445026398, val_loss 0.15770769119262695 \n","    Record loss - saving at epoch 280\n","    epoch 280, train_loss 0.18692095577716827, val_loss 0.1536172330379486 \n","    Record loss - saving at epoch 292\n","    epoch 292, train_loss 0.18613967299461365, val_loss 0.15274935960769653 \n","    Record loss - saving at epoch 300\n","    epoch 300, train_loss 0.18132033944129944, val_loss 0.15175306797027588 \n","epoch 300, train_loss 0.18132033944129944, val_loss 0.15175306797027588 \n","    Record loss - saving at epoch 301\n","    epoch 301, train_loss 0.18041710555553436, val_loss 0.15144167840480804 \n","    Record loss - saving at epoch 302\n","    epoch 302, train_loss 0.18217262625694275, val_loss 0.14728626608848572 \n","    Record loss - saving at epoch 328\n","    epoch 328, train_loss 0.1735001802444458, val_loss 0.14224056899547577 \n","    Record loss - saving at epoch 338\n","    epoch 338, train_loss 0.17034605145454407, val_loss 0.1412498652935028 \n","    Record loss - saving at epoch 339\n","    epoch 339, train_loss 0.16625413298606873, val_loss 0.13955986499786377 \n","    Record loss - saving at epoch 340\n","    epoch 340, train_loss 0.16576246917247772, val_loss 0.13804186880588531 \n","epoch 350, train_loss 0.16376419365406036, val_loss 0.1579098254442215 \n","    Record loss - saving at epoch 360\n","    epoch 360, train_loss 0.16078411042690277, val_loss 0.1333187371492386 \n","    Record loss - saving at epoch 366\n","    epoch 366, train_loss 0.15777038037776947, val_loss 0.13247543573379517 \n","    Record loss - saving at epoch 369\n","    epoch 369, train_loss 0.15515293180942535, val_loss 0.1296907514333725 \n","    Record loss - saving at epoch 372\n","    epoch 372, train_loss 0.15445944666862488, val_loss 0.1274024248123169 \n","    Record loss - saving at epoch 380\n","    epoch 380, train_loss 0.15001694858074188, val_loss 0.12387459725141525 \n","    Record loss - saving at epoch 381\n","    epoch 381, train_loss 0.1507282853126526, val_loss 0.12194204330444336 \n","    Record loss - saving at epoch 385\n","    epoch 385, train_loss 0.14859899878501892, val_loss 0.11928920447826385 \n","    Record loss - saving at epoch 389\n","    epoch 389, train_loss 0.14485293626785278, val_loss 0.1187402606010437 \n","    Record loss - saving at epoch 390\n","    epoch 390, train_loss 0.1448657214641571, val_loss 0.11592534929513931 \n","    Record loss - saving at epoch 395\n","    epoch 395, train_loss 0.13951195776462555, val_loss 0.1098695620894432 \n","epoch 400, train_loss 0.13896718621253967, val_loss 0.11406943202018738 \n","    Record loss - saving at epoch 403\n","    epoch 403, train_loss 0.13849513232707977, val_loss 0.10613465309143066 \n","    Record loss - saving at epoch 405\n","    epoch 405, train_loss 0.1375221610069275, val_loss 0.10371171683073044 \n","    Record loss - saving at epoch 409\n","    epoch 409, train_loss 0.1342107653617859, val_loss 0.10311533510684967 \n","    Record loss - saving at epoch 410\n","    epoch 410, train_loss 0.13072721660137177, val_loss 0.09998228400945663 \n","    Record loss - saving at epoch 418\n","    epoch 418, train_loss 0.1300949901342392, val_loss 0.09883368015289307 \n","    Record loss - saving at epoch 419\n","    epoch 419, train_loss 0.12593308091163635, val_loss 0.09739851951599121 \n","    Record loss - saving at epoch 421\n","    epoch 421, train_loss 0.12792067229747772, val_loss 0.09455771744251251 \n","    Record loss - saving at epoch 430\n","    epoch 430, train_loss 0.12309868633747101, val_loss 0.09373348206281662 \n","    Record loss - saving at epoch 433\n","    epoch 433, train_loss 0.12252803891897202, val_loss 0.09250938147306442 \n","    Record loss - saving at epoch 434\n","    epoch 434, train_loss 0.12123368680477142, val_loss 0.09134843945503235 \n","    Record loss - saving at epoch 440\n","    epoch 440, train_loss 0.1216578334569931, val_loss 0.08809959143400192 \n","    Record loss - saving at epoch 449\n","    epoch 449, train_loss 0.11779464781284332, val_loss 0.08748970925807953 \n","epoch 450, train_loss 0.11648605018854141, val_loss 0.09186174720525742 \n","    Record loss - saving at epoch 452\n","    epoch 452, train_loss 0.1187279224395752, val_loss 0.08582796901464462 \n","    Record loss - saving at epoch 453\n","    epoch 453, train_loss 0.11608334630727768, val_loss 0.08541875332593918 \n","    Record loss - saving at epoch 457\n","    epoch 457, train_loss 0.11514143645763397, val_loss 0.08493495732545853 \n","    Record loss - saving at epoch 461\n","    epoch 461, train_loss 0.11381979286670685, val_loss 0.08404023200273514 \n","    Record loss - saving at epoch 462\n","    epoch 462, train_loss 0.11320462077856064, val_loss 0.08345884829759598 \n","    Record loss - saving at epoch 467\n","    epoch 467, train_loss 0.11506865173578262, val_loss 0.08326581865549088 \n","    Record loss - saving at epoch 469\n","    epoch 469, train_loss 0.11293105036020279, val_loss 0.08298633247613907 \n","    Record loss - saving at epoch 470\n","    epoch 470, train_loss 0.11246631294488907, val_loss 0.08230803161859512 \n","    Record loss - saving at epoch 471\n","    epoch 471, train_loss 0.11375726759433746, val_loss 0.08157315850257874 \n","    Record loss - saving at epoch 479\n","    epoch 479, train_loss 0.1105741634964943, val_loss 0.07770811766386032 \n","    Record loss - saving at epoch 494\n","    epoch 494, train_loss 0.10866820812225342, val_loss 0.07542532682418823 \n","epoch 500, train_loss 0.10751010477542877, val_loss 0.07612831890583038 \n","    Record loss - saving at epoch 508\n","    epoch 508, train_loss 0.10700410604476929, val_loss 0.07475828379392624 \n","    Record loss - saving at epoch 511\n","    epoch 511, train_loss 0.10475210845470428, val_loss 0.07348743826150894 \n","    Record loss - saving at epoch 518\n","    epoch 518, train_loss 0.10601519048213959, val_loss 0.07343097031116486 \n","    Record loss - saving at epoch 531\n","    epoch 531, train_loss 0.10433226823806763, val_loss 0.0707063302397728 \n","epoch 550, train_loss 0.10416529327630997, val_loss 0.07227521389722824 \n","    Record loss - saving at epoch 561\n","    epoch 561, train_loss 0.10272513329982758, val_loss 0.07008329033851624 \n","    Record loss - saving at epoch 562\n","    epoch 562, train_loss 0.10202449560165405, val_loss 0.06918814033269882 \n","    Record loss - saving at epoch 563\n","    epoch 563, train_loss 0.09912768006324768, val_loss 0.06809201091527939 \n","    Record loss - saving at epoch 585\n","    epoch 585, train_loss 0.09766208380460739, val_loss 0.06651154160499573 \n","epoch 600, train_loss 0.0968526154756546, val_loss 0.06947873532772064 \n","    Record loss - saving at epoch 615\n","    epoch 615, train_loss 0.09403257071971893, val_loss 0.06626478582620621 \n","    Record loss - saving at epoch 622\n","    epoch 622, train_loss 0.09565626829862595, val_loss 0.0661620944738388 \n","    Record loss - saving at epoch 624\n","    epoch 624, train_loss 0.09551869332790375, val_loss 0.06542529910802841 \n","    Record loss - saving at epoch 646\n","    epoch 646, train_loss 0.09250510483980179, val_loss 0.06458824127912521 \n","    Record loss - saving at epoch 650\n","    epoch 650, train_loss 0.09343951940536499, val_loss 0.0625954195857048 \n","epoch 650, train_loss 0.09343951940536499, val_loss 0.0625954195857048 \n","    Record loss - saving at epoch 669\n","    epoch 669, train_loss 0.09269930422306061, val_loss 0.06236734241247177 \n","    Record loss - saving at epoch 674\n","    epoch 674, train_loss 0.09085364639759064, val_loss 0.06236692890524864 \n","    Record loss - saving at epoch 681\n","    epoch 681, train_loss 0.09080880135297775, val_loss 0.061159469187259674 \n","    Record loss - saving at epoch 684\n","    epoch 684, train_loss 0.09092022478580475, val_loss 0.060749251395463943 \n","epoch 700, train_loss 0.09037955850362778, val_loss 0.06458170711994171 \n","    Record loss - saving at epoch 724\n","    epoch 724, train_loss 0.08950621634721756, val_loss 0.060215502977371216 \n","    Record loss - saving at epoch 747\n","    epoch 747, train_loss 0.08665858209133148, val_loss 0.05939917266368866 \n","epoch 750, train_loss 0.08868740499019623, val_loss 0.0646730363368988 \n","    Record loss - saving at epoch 757\n","    epoch 757, train_loss 0.08755216747522354, val_loss 0.05897389352321625 \n","    Record loss - saving at epoch 775\n","    epoch 775, train_loss 0.08490459620952606, val_loss 0.058830901980400085 \n","    Record loss - saving at epoch 787\n","    epoch 787, train_loss 0.08439058065414429, val_loss 0.05786585062742233 \n","    Record loss - saving at epoch 794\n","    epoch 794, train_loss 0.08714277297258377, val_loss 0.05772078037261963 \n","    Record loss - saving at epoch 800\n","    epoch 800, train_loss 0.08582000434398651, val_loss 0.057362355291843414 \n","epoch 800, train_loss 0.08582000434398651, val_loss 0.057362355291843414 \n","    Record loss - saving at epoch 820\n","    epoch 820, train_loss 0.08399040997028351, val_loss 0.05673583969473839 \n","    Record loss - saving at epoch 844\n","    epoch 844, train_loss 0.08286117762327194, val_loss 0.056571636348962784 \n","    Record loss - saving at epoch 846\n","    epoch 846, train_loss 0.08307261765003204, val_loss 0.05613260343670845 \n","epoch 850, train_loss 0.08291825652122498, val_loss 0.0594756044447422 \n","    Record loss - saving at epoch 861\n","    epoch 861, train_loss 0.0823754072189331, val_loss 0.05607571452856064 \n","    Record loss - saving at epoch 886\n","    epoch 886, train_loss 0.08127729594707489, val_loss 0.055219616740942 \n","    Record loss - saving at epoch 887\n","    epoch 887, train_loss 0.08135974407196045, val_loss 0.05460276082158089 \n","epoch 900, train_loss 0.2647853195667267, val_loss 0.15169699490070343 \n","epoch 950, train_loss 0.08167169988155365, val_loss 0.058971796184778214 \n","    Record loss - saving at epoch 953\n","    epoch 953, train_loss 0.08054062724113464, val_loss 0.05443277955055237 \n","    Record loss - saving at epoch 974\n","    epoch 974, train_loss 0.07910054922103882, val_loss 0.054255835711956024 \n","    Record loss - saving at epoch 977\n","    epoch 977, train_loss 0.07976923137903214, val_loss 0.05398565158247948 \n","    Record loss - saving at epoch 993\n","    epoch 993, train_loss 0.07940433919429779, val_loss 0.05318352207541466 \n","    Record loss - saving at epoch 996\n","    epoch 996, train_loss 0.07875382155179977, val_loss 0.052580468356609344 \n","    Record loss - saving at epoch 997\n","    epoch 997, train_loss 0.07870928943157196, val_loss 0.05246606469154358 \n","epoch 1000, train_loss 0.0791943296790123, val_loss 0.05989455059170723 \n","    Record loss - saving at epoch 1013\n","    epoch 1013, train_loss 0.07845389097929001, val_loss 0.0522838793694973 \n","    Record loss - saving at epoch 1016\n","    epoch 1016, train_loss 0.07748664915561676, val_loss 0.05176185071468353 \n","epoch 1050, train_loss 0.0764288455247879, val_loss 0.05344831943511963 \n","    Record loss - saving at epoch 1067\n","    epoch 1067, train_loss 0.07776442170143127, val_loss 0.05087646469473839 \n","    Record loss - saving at epoch 1072\n","    epoch 1072, train_loss 0.07727701216936111, val_loss 0.04981071501970291 \n","    Record loss - saving at epoch 1093\n","    epoch 1093, train_loss 0.07618274539709091, val_loss 0.04911888763308525 \n","epoch 1100, train_loss 0.07823881506919861, val_loss 0.05072460696101189 \n","    Record loss - saving at epoch 1104\n","    epoch 1104, train_loss 0.07475707679986954, val_loss 0.048731159418821335 \n","    Record loss - saving at epoch 1142\n","    epoch 1142, train_loss 0.07461096346378326, val_loss 0.04861150681972504 \n","epoch 1150, train_loss 0.07554206997156143, val_loss 0.05095851048827171 \n","    Record loss - saving at epoch 1157\n","    epoch 1157, train_loss 0.07445143908262253, val_loss 0.04847516119480133 \n","    Record loss - saving at epoch 1178\n","    epoch 1178, train_loss 0.0746239572763443, val_loss 0.048159174621105194 \n","    Record loss - saving at epoch 1197\n","    epoch 1197, train_loss 0.0751255601644516, val_loss 0.04728572815656662 \n","epoch 1200, train_loss 0.07482898980379105, val_loss 0.04991784319281578 \n","    Record loss - saving at epoch 1224\n","    epoch 1224, train_loss 0.0737132579088211, val_loss 0.046676941215991974 \n","epoch 1250, train_loss 0.07390223443508148, val_loss 0.049362074583768845 \n","epoch 1300, train_loss 0.09651835262775421, val_loss 0.06460463255643845 \n","    Record loss - saving at epoch 1318\n","    epoch 1318, train_loss 0.07279083877801895, val_loss 0.046503182500600815 \n","    Record loss - saving at epoch 1320\n","    epoch 1320, train_loss 0.0717596635222435, val_loss 0.04604052007198334 \n","    Record loss - saving at epoch 1348\n","    epoch 1348, train_loss 0.07068859040737152, val_loss 0.04520449787378311 \n","epoch 1350, train_loss 0.07213791459798813, val_loss 0.04665176942944527 \n","    Record loss - saving at epoch 1381\n","    epoch 1381, train_loss 0.07043461501598358, val_loss 0.0450909286737442 \n","epoch 1400, train_loss 0.07125546038150787, val_loss 0.04707987606525421 \n","    Record loss - saving at epoch 1418\n","    epoch 1418, train_loss 0.07034064829349518, val_loss 0.04491002485156059 \n","    Record loss - saving at epoch 1432\n","    epoch 1432, train_loss 0.0700659304857254, val_loss 0.04464293643832207 \n","    Record loss - saving at epoch 1450\n","    epoch 1450, train_loss 0.070004902780056, val_loss 0.04326823726296425 \n","epoch 1450, train_loss 0.070004902780056, val_loss 0.04326823726296425 \n","epoch 1500, train_loss 0.06937148422002792, val_loss 0.04828345775604248 \n","epoch 1550, train_loss 0.07067318260669708, val_loss 0.04626881703734398 \n","Epoch 01552: reducing learning rate of group 0 to 2.5000e-03.\n","    Record loss - saving at epoch 1555\n","    epoch 1555, train_loss 0.06700160354375839, val_loss 0.04267221316695213 \n","    Record loss - saving at epoch 1558\n","    epoch 1558, train_loss 0.06672108918428421, val_loss 0.042603082954883575 \n","    Record loss - saving at epoch 1560\n","    epoch 1560, train_loss 0.06693406403064728, val_loss 0.042326170951128006 \n","    Record loss - saving at epoch 1561\n","    epoch 1561, train_loss 0.06605826318264008, val_loss 0.0419347807765007 \n","    Record loss - saving at epoch 1563\n","    epoch 1563, train_loss 0.0670757070183754, val_loss 0.04123630002140999 \n","    Record loss - saving at epoch 1566\n","    epoch 1566, train_loss 0.06648800522089005, val_loss 0.0409078374505043 \n","epoch 1600, train_loss 0.06706811487674713, val_loss 0.04621253162622452 \n","    Record loss - saving at epoch 1606\n","    epoch 1606, train_loss 0.06705360114574432, val_loss 0.0408523790538311 \n","    Record loss - saving at epoch 1647\n","    epoch 1647, train_loss 0.06639891862869263, val_loss 0.04070529341697693 \n","epoch 1650, train_loss 0.06606577336788177, val_loss 0.042148277163505554 \n","epoch 1700, train_loss 0.0660051479935646, val_loss 0.04503658041357994 \n","Epoch 01749: reducing learning rate of group 0 to 1.2500e-03.\n","epoch 1750, train_loss 0.06448274105787277, val_loss 0.04135480895638466 \n","    Record loss - saving at epoch 1752\n","    epoch 1752, train_loss 0.06445542722940445, val_loss 0.04024543613195419 \n","    Record loss - saving at epoch 1754\n","    epoch 1754, train_loss 0.0647013932466507, val_loss 0.04013404995203018 \n","    Record loss - saving at epoch 1762\n","    epoch 1762, train_loss 0.06442813575267792, val_loss 0.04000654071569443 \n","    Record loss - saving at epoch 1772\n","    epoch 1772, train_loss 0.06442318111658096, val_loss 0.03963974490761757 \n","epoch 1800, train_loss 0.06460049748420715, val_loss 0.040841735899448395 \n","    Record loss - saving at epoch 1804\n","    epoch 1804, train_loss 0.06418945640325546, val_loss 0.0394301563501358 \n","epoch 1850, train_loss 0.06474259495735168, val_loss 0.04154420271515846 \n","epoch 1900, train_loss 0.06453021615743637, val_loss 0.040408771485090256 \n","Epoch 01906: reducing learning rate of group 0 to 6.2500e-04.\n","    Record loss - saving at epoch 1913\n","    epoch 1913, train_loss 0.06371554732322693, val_loss 0.03940011188387871 \n","    Record loss - saving at epoch 1925\n","    epoch 1925, train_loss 0.06375475227832794, val_loss 0.03922043740749359 \n","epoch 1950, train_loss 0.06348772346973419, val_loss 0.03985011950135231 \n","epoch 2000, train_loss 0.06371535360813141, val_loss 0.03994119539856911 \n","Epoch 02027: reducing learning rate of group 0 to 3.1250e-04.\n","    Record loss - saving at epoch 2037\n","    epoch 2037, train_loss 0.06303225457668304, val_loss 0.03919566050171852 \n","epoch 2050, train_loss 0.06312279403209686, val_loss 0.039810143411159515 \n","epoch 2100, train_loss 0.06280934810638428, val_loss 0.04005642235279083 \n","Epoch 02139: reducing learning rate of group 0 to 1.5625e-04.\n","epoch 2150, train_loss 0.06281265616416931, val_loss 0.03939305618405342 \n","    Record loss - saving at epoch 2182\n","    epoch 2182, train_loss 0.06264565885066986, val_loss 0.03917340934276581 \n","epoch 2200, train_loss 0.06281204521656036, val_loss 0.039651788771152496 \n","epoch 2250, train_loss 0.06252074241638184, val_loss 0.03951485455036163 \n","Epoch 02284: reducing learning rate of group 0 to 7.8125e-05.\n","epoch 2300, train_loss 0.06252080947160721, val_loss 0.03931960090994835 \n","epoch 2350, train_loss 0.06256291270256042, val_loss 0.039583705365657806 \n","Epoch 02385: reducing learning rate of group 0 to 3.9063e-05.\n","epoch 2400, train_loss 0.0623294822871685, val_loss 0.03936045616865158 \n","epoch 2450, train_loss 0.06252531707286835, val_loss 0.03935305029153824 \n","Epoch 02486: reducing learning rate of group 0 to 1.9531e-05.\n","epoch 2500, train_loss 0.06227532774209976, val_loss 0.03934812918305397 \n","epoch 2550, train_loss 0.06243418902158737, val_loss 0.039363790303468704 \n","Epoch 02587: reducing learning rate of group 0 to 9.7656e-06.\n","epoch 2600, train_loss 0.061717014759778976, val_loss 0.03938152641057968 \n","epoch 2650, train_loss 0.062454741448163986, val_loss 0.039359670132398605 \n","max patience reached, stopping training\n"]}],"source":["# training loop\n","lowest_val_loss = 0\n","best_loss = False\n","\n","max_patience = 500\n","curr_patience = max_patience\n","\n","# datetime object containing current date and time\n","best_epoch = 0\n","now = datetime.now()\n","dt_string = now.strftime(f\"models/{model.model_type}_%d-%m-%Y_%H-%M-%S\")\n","\n","os.mkdir(os.path.join(\".\", dt_string))\n","\n","with open(f'{dt_string}/model_structure.txt', \"w\") as model_structure:\n","    model_structure.write(str(model))\n","\n","for epoch in range(max_epochs):\n","    #ep_loss = myk_train.train_epoch_interval(model, train_dl, loss_functions, optimiser, device=device)\n","    ep_loss = myk_train.train_epoch_interval(model, train_dl, loss_functions, optimiser, device=device)\n","\n","    #ep_loss = myk_train.train_epoch(model, train_dl, loss_functions, optimiser, device=device)\n","    val_loss = myk_train.compute_batch_loss(model, val_dl, loss_functions, device=device)\n","\n","    scheduler.step(val_loss)\n","\n","    writer.add_scalar(\"Loss/val\", val_loss, epoch)\n","    writer.add_scalar(\"Loss/train\", ep_loss, epoch)\n","\n","    # check if we have beaten our best loss to date\n","    if lowest_val_loss == 0: # first run\n","        lowest_val_loss = val_loss\n","    elif val_loss < lowest_val_loss:# new record\n","        lowest_val_loss = val_loss\n","        best_loss = True\n","    else: # no improvement\n","        best_loss = False\n","        curr_patience -= 1\n","\n","    if best_loss: # save best model so far\n","        best_epoch = epoch\n","        print(f\"    Record loss - saving at epoch {epoch}\")\n","        # save for RTNeural\n","        model.save_for_rtneural(f\"{dt_string}/model.json\")\n","        # save for pythorch\n","        torch.save(model.state_dict(), f\"{dt_string}/model.ph\")\n","        print(f\"    epoch {epoch}, train_loss {ep_loss}, val_loss {val_loss} \")\n","        curr_patience = max_patience\n","    if epoch % 50 == 0: # save an example processed audio file\n","        myk_evaluate.run_file_through_model(model, test_file, audio_folder + \"/\" + run_name + str(epoch)+\".wav\")\n","        print(f\"epoch {epoch}, train_loss {ep_loss}, val_loss {val_loss} \")\n","    if curr_patience == 0:\n","        print(\"max patience reached, stopping training\")\n","        # load best parameters in the model\n","        model.load_state_dict(torch.load(f\"{dt_string}/model.ph\"))\n","        model.eval() # set inference state in the possible layers that need it\n","        myk_evaluate.run_file_through_model(model, test_file, audio_folder + \"/\" + run_name + str(best_epoch)+\"_BEST.wav\")\n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"edQeVKnKqIlY"},"outputs":[],"source":["to_append = [f'\\n{run_name}/*.wav', f'\\n!{run_name}/*_BEST*.wav']\n","\n","with open(f'{audio_folder}/../.gitignore', \"r+\") as gitignore:\n","    read = gitignore.read()\n","    if to_append[0][2:len(to_append[0])] not in read:\n","        gitignore.writelines(to_append)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1689952881105,"user":{"displayName":"FireHead Vlogs","userId":"00714204215770433149"},"user_tz":-120},"id":"3wYTWml9kSD1","outputId":"07449eb4-226a-417a-f6a3-8420aed531fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["updating: models/LSTM_21-07-2023_15-03-43/ (stored 0%)\n","  adding: models/LSTM_21-07-2023_15-03-43/model_structure.txt (deflated 19%)\n","  adding: models/LSTM_21-07-2023_15-03-43/model.json (deflated 53%)\n","  adding: models/LSTM_21-07-2023_15-03-43/model.ph (deflated 16%)\n"]}],"source":["!zip -r 'test16.zip' './models/LSTM_21-07-2023_15-03-43/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dsDOtP34CPeS"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1R90W11c0RiDl3RfZT-sX8pRNVVVbmSYh","timestamp":1685016693193}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":0}
